{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Python Free-Threading Guide","text":"<p>Free-threaded CPython is here!  </p>"},{"location":"#purpose-of-this-site","title":"Purpose of this site","text":"<p>This website serves as a centralized resource for Python package maintainers, as well as end users, interested in supporting or experimenting with free-threaded Python.</p> <p>It also tracks the compatibility status of packages that include native code here:</p> <ul> <li>Free-threading Python Compatibility Status Tracker</li> </ul> <p>It offers documentation and guidance for setting up a free-threaded Python development environment and getting code working under the free-threaded build.</p> <p>Lysandros Nikolaou and Nathan Goldbaum presented a talk at PyCon 2025 based on content covered by this guide. You can see a recording of the talk on YouTube.</p>"},{"location":"#status-of-free-threaded-python","title":"Status of Free-Threaded Python","text":"<p>After the acceptance by the Python Steering Council of PEP 703 - Making the Global Interpreter Lock Optional in CPython, CPython 3.13 shipped an experimental free-threaded build. With the acceptance of PEP 779 - Criteria for supported status for free-threaded Python, the free-threaded interpreter is no longer considered experimental starting in Python 3.14, although it is not yet the default interpreter build.</p> <p>Work is underway across the ecosystem to update packages to support free-threaded Python alongside improvements to thread safety guarantees, testing, and documentation.</p>"},{"location":"#install-free-threaded-python","title":"Install free-threaded Python","text":"<p>The following sections describe how to install free-threaded Python, run it, and verify the GIL is disabled.</p> <ul> <li>Installing Free-Threaded Python</li> <li>Running Python with the GIL Disabled</li> </ul>"},{"location":"#why-is-free-threaded-python-exciting","title":"Why is free-threaded Python exciting?","text":"<p>We've put together a few example multithreaded Python applications where free-threaded Python is substantially faster than the GIL-enabled build.</p> <ul> <li>Examples for How to Use Free-Threaded Python in an Application</li> </ul>"},{"location":"#run-your-code-on-free-threaded-python","title":"Run your code on free-threaded Python","text":"<p>If you are interested in updating your code to work on free-threaded Python, your next steps depend on your needs and goals. To begin, select the option that best describes you:</p> I am writing a Python script or application that uses Python libraries I don't maintain <p>You should start experimenting with free-threaded Python once the libraries you depend on advertise support for free-threading. See the tracking table for more details about the status of free-threaded support in Python libraries.</p> <p>If your dependencies advertise free-threaded support, good news! If you do not use the <code>threading</code> module and do not plan to, then you're done and you can feel safe declaring support for running your project on the free-threaded build.</p> <p>If you would like to use the <code>threading</code> module to improve the performance of your project, you should read the documentation of your dependencies and learn about their thread safety guarantees. This is particularly true of libraries that expose mutable objects, doubly so if you want to mutate a shared object from many threads.</p> <p>Pure Python code can exhibit thread safety issues, so you may also want to look at the first section of the porting guide, particularly on the thread safety of pure Python code:</p> <ul> <li>Porting Python Packages to Support Free-Threading</li> </ul> I maintain a pure Python app or tool written in pure Python with no public Python API <p>You should start experimenting with free-threaded Python once the libraries you depend on advertise support for free-threading. See the tracking table for more details about the status of free-threaded support in Python libraries.</p> <p>If your dependencies advertise free-threaded support, good news! If you do not use the <code>threading</code> module and do not plan to, then you're done and you can feel safe declaring support for running your project on the free-threaded build.</p> <p>If you make use of the <code>threading</code> module internally and already have multithreaded tests, consider experimenting with your existing tests with a very short thread switch interval. This can elicit thread safety issues on the GIL-enabled build. If you do not use <code>threading</code> or thread pools internally your tool or app should behave identically under free-threading.</p> <p>If you would like to use the <code>threading</code> module to improve the performance of your project, you should read the documentation of your dependencies and learn about their thread safety guarantees. This is particularly true of libraries that expose mutable objects, doubly so if you want to mutate a shared object from many threads.</p> <p>Pure Python code can exhibit thread safety issues, so you may also want to look at the first section of the porting guide, particularly on the thread safety of pure Python code:</p> <ul> <li>Porting Python Packages to Support Free-Threading</li> </ul> I maintain a pure Python package with a public Python API <p>Free-threading is implemented in CPython such that pure Python code is thread-safe, at least to the same extent as it is with the GIL enabled today. We use \"thread-safe\" here to mean that CPython should not crash running multithreaded pure Python code, not necessarily that a multithreaded program will always produce deterministic results, even if the GIL-enabled build is deterministic. It is up to the author of a program, application, or library to ensure safe multithreaded usage when using the library in a supported manner.</p> <p>There are a few ways you can create thread safety issues in your own code. The most common ones are: using global state for configuration or other purposes, implementing a cache with a dict or other variable not meant for that purpose, or using functionality of a dependency that itself isn't thread-safe. You should also think about whether you would like to support multithreaded use of any mutable data structures exposed by your package. If your package does none of those things, you are very likely ready for free-threading already.</p> <p>What gets trickier is testing whether your package is thread-safe. For that you'll need multi-threaded tests, and that can be more involved - see our guide to adding multithreaded test coverage to Python packages.</p> <ul> <li>Porting Python Packages to Support Free-Threading</li> <li>Improving Multithreaded Test Coverage</li> </ul> I maintain a Python package with compiled extension modules <p>As usual with extensions, dealing with native code will take some work but we hope that this guide will provide you with a toolkit to get things working.</p> <p>We suggest reading through the the full porting guide, including the final section that focuses on considerations for native code.</p> <ul> <li>Porting Python Packages to Support Free-Threading</li> <li>Improving Multithreaded Test Coverage</li> <li>Updating Native Extensions to Support Free-Threading</li> </ul>"},{"location":"#frequently-asked-questions-about-errors","title":"Frequently asked questions about errors","text":"<ul> <li>Frequently seen errors and how to fix them</li> </ul>"},{"location":"#advanced-topics-for-package-maintainers","title":"Advanced topics for package maintainers","text":"<ul> <li>Setting up Continuous Integration</li> <li>Debugging Thread Safety Issues</li> <li>Multithreaded Profiling</li> </ul>"},{"location":"#further-reading","title":"Further reading","text":"<p>We've collected additional resources on free-threaded Python and multithreaded programming here:</p> <ul> <li>More Resources</li> </ul>"},{"location":"#news-and-getting-help","title":"News and getting help","text":"<p>You can join the Free-threaded Python Community Discord using this invite link: https://discord.gg/rqgHCDqdRr.</p> <p>You can also ask questions in the Threading category of the Python community Discourse forum.</p> <p>Please read and be mindful of community rules. Both communities enforce the Python Software Foundation Code of Conduct.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome your contributions. Please read the Contributing page for more details. Please open issues or pull requests on this repo for anything that seems in scope for this site or for tracking issues related to support for free-threaded Python across the ecosystem.</p>"},{"location":"#about-this-site","title":"About this site","text":"<p>This site is maintained primarily by Quansight Labs, where a team is working together with the Python runtime team at Meta and stakeholders across the ecosystem to jumpstart work on converting the libraries that make up the scientific Python and AI/ML stacks to work with the free-threaded build of CPython 3.13. Additionally, that effort will look at libraries like PyO3 that are needed to interface with CPython from other languages.</p>"},{"location":"ci/","title":"Setting up CI","text":""},{"location":"ci/#ci-setup-via-setup-python","title":"CI setup via <code>setup-python</code>","text":"<p>The easiest way to get a free-threaded Python build on a CI runner is with the <code>setup-python</code> Github Action:</p> <pre><code>jobs:\n  free-threaded:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@...\n      - uses: actions/setup-python@...\n        with:\n          python-version: 3.13t\n</code></pre>"},{"location":"ci/#ci-setup-via-setup-uv","title":"CI setup via <code>setup-uv</code>","text":"<p>An alternative to <code>setup-python</code> is to use <code>setup-uv</code> Github Action:</p> <pre><code>jobs:\n  free-threaded:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@...\n      - uses: astral-sh/setup-uv@...\n        with:\n          python-version: 3.13t\n</code></pre> <p>You should replace the ellipses with versions for the actions.</p>"},{"location":"ci/#windows-ci-setup-via-custom-powershell","title":"Windows CI setup via custom PowerShell","text":"<p>For installing a free-threaded build of Python on a Windows CI runner (<code>runs-on: windows-latest</code>), you can download and install directly from https://www.python.org/ftp/python/ as shown in the following PowerShell snippet (noting that the free-threaded binary is named <code>python{version}t.exe</code>, where the \"t\" is for free-\"t\"hreaded). For more tips see the docs on silent installation and options on Windows.</p> <pre><code>jobs:\n  free-threaded:\n    runs-on: windows-latest\n    steps:\n      - uses: actions/checkout@...\n      - name: custom python install script\n        shell: pwsh\n        run: |\n          $pythonInstallerUrl = 'https://www.nuget.org/api/v2/package/python-freethreaded/3.13.1'\n          Invoke-WebRequest $pythonInstallerUrl -OutFile 'python-freethreaded.3.13.1.nupkg'\n          Install-Package python-freethreaded -Scope CurrentUser -Source $pwd\n          $python_dir = (Get-Item((Get-Package -Name python-freethreaded).Source)).DirectoryName\n          $env:path = $python_dir + \"\\tools;\" + $python_dir + \"\\tools\\Scripts;\" + $env:Path\n</code></pre>"},{"location":"ci/#building-free-threaded-wheels-with-cibuildwheel","title":"Building free-threaded wheels with cibuildwheel","text":"<p>cibuildwheel has support for building free-threaded wheels on all platforms. If your project releases nightly wheels, we suggest configuring <code>cibuildwheel</code> to build nightly free-threaded wheels.</p> <p>To ensure wheels are built correctly under cibuildwheel, you will need to specify the following variables in the environment for the cibuildwheel action:</p> <pre><code>  - name: Build wheels\n    uses: pypa/cibuildwheel@...\n    env:\n      CIBW_ENABLE: cpython-freethreading\n      CIBW_BUILD: cp313t-${{ matrix.buildplat }}\n</code></pre> <p>As above, replace the ellipses with a <code>cibuildwheel</code> version.</p> <p>You will also likely need to manually pass <code>-Xgil=0</code> or set <code>PYTHON_GIL=0</code> in your shell environment while running tests to ensure the GIL is actually disabled during tests, at least until you can register that your extension modules support disabling the GIL via <code>Py_mod_gil</code> and all of your runtime test dependencies do the same. See the porting guide for more information about declaring support for free-threaded python in your extension.</p> <p>Info</p> <p>If a dependency of your package does not support free-threading or has not yet done a release which includes <code>cp313t</code> wheels, this can be tricky to work around because an environment marker for free-threading does not exist (see this Discourse thread). Hence it is not possible to special-case free-threading with static metadata in <code>pyproject.toml</code>. It's fine to still upload <code>cp313t</code> wheels for your package to PyPI; the user may then be responsible for getting the dependency installed (e.g., from a nightly wheel or building the dependency's <code>main</code> branch from source) if the last release of the dependency doesn't cleanly build from source or doesn't work under free-threading.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>We very much welcome ideas, questions and relevant contributions from anyone with an interest in improving the adoption and usage of free-threaded CPython.</p> <p>Please read the Code of Conduct and be mindful of the community rules while working on the project.</p>"},{"location":"contributing/#contributing-to-this-site","title":"Contributing to this site","text":"<p>Contributions can be made through issues on and pull requests to the free-threaded-compatibility repository. Ways to contribute include:</p> <ul> <li>Improvements to the documentation, from new content to copy-editing for clarity.</li> <li>Updates to the status tracker.</li> <li>Add links to relevant examples, blog posts and other content.</li> <li>Create example use cases or benchmarks and add them to the listing of examples.</li> </ul>"},{"location":"contributing/#contributing-to-the-adoption-of-free-threading","title":"Contributing to the adoption of free-threading","text":"<p>There's a ton of work to do here - and a ton of fun and performance to be had! This is perhaps the single most impactful and exciting change to CPython since the Python 2 to Python 3 transition. It's going to take the community multiple years to complete the transition to free-threaded CPython, and your help will be valuable. Here are some ways to contribute today:</p> <ol> <li>Start experimenting! If the packages you rely on for some task or application,     try running it with a free-threaded interpreter. And consider whether it can     benefit performance or functionality-wise from Python-level threading.</li> <li>Implement support for free-threading in packages that are still lacking     support. We'll try to provide an outline of how to approach that in an open     source package below.</li> </ol>"},{"location":"contributing/#news-and-getting-help","title":"News and getting help","text":"<p>You can join the Free-threaded Python Community Discord using this invite link: https://discord.gg/rqgHCDqdRr.</p> <p>You can also ask questions in the Threading category of the Python community Discourse forum.</p> <p>Please read and be mindful of community rules. Both communities enforce the Python Software Foundation Code of Conduct.</p>"},{"location":"contributing/#implementing-support-for-free-threading-in-a-package","title":"Implementing support for free-threading in a package","text":"<p>A good place to start is to check the current status of the package. Are there docs on free-threading support? Does the issue tracker for the package have an issue about adding free-threading support? If not, is there a relevant PR? Typically, searching for \"free-threading\", \"free-threaded\", \"GIL\", \"3.13t\" and \"cp313t\" will allow you to find the relevant issue or PR if it exists.</p> <p>If there is no issue yet and you want to contribute support, opening an issue is usually a good next step (please check the projects contribution guidelines for details on how the maintainers want you to suggest a new feature). Here is example content for such an issue:</p> <pre><code>Title: *Support for free-threaded CPython*\n\nI am interested in adding support for free-threading to PROJECT-NAME. I had a\nlook at what it would take to implement that.\n\n&lt;!-- INSERT YOUR THOUGHTS SPECIFIC TO THE PACKAGE HERE:\n- Are there relevant threading-related issues or docs?\n- If there are native libraries (C, C++, etc.), are they thread-safe? Does code\n  in the package already drop the GIL?\n- Is there relevant global state?\n- Are there particular parts of the code base for which dedicated\n  multi-threaded tests should be written?\n--&gt;\n\nThe standard TODOs for adding free-threading support are:\n\n- [ ] Audit Python bindings and declare them free-threading compatible (xref https://py-free-threading.github.io/porting/#updating-extension-modules).\n- [ ] Run the test suite with `pytest-run-parallel` to find potential issues, and fix them.\n- [ ] Run the test suite under ThreadSanitizer. _If possible, depends on how many dependencies there are and if they run under TSan._\n- [ ] Add `cp313t-*` to CI to build free-threading wheels.\n\nFor more details, please see the\n[suggested plan of attack in the py-free-threading guide](https://py-free-threading.github.io/porting/#suggested-plan-of-attack).\n\nNote that this is the first time I've looked at this repo, so I might be\nmissing known issues or code that needs closer inspection. Any suggestions here\nwill be very useful.\n\nI will be happy to help and work on this. Please do let me know if you'd prefer\nme to hold off for any reason.\n</code></pre>"},{"location":"debugging/","title":"Debugging thread safety issues","text":"<p>Until now, the GIL has allowed developers to ignore C safety issues when writing parallel programs, since the GIL ensured that all thread execution was serialized, allowing for simultaneous access to Python objects and state defined in the interpreter.</p> <p>The new free-threaded model ensures that Python code access originating from other Python code frames is safe and is guaranteed to not produce any major interpreter crash, as opposed to unrestricted C code access, which can present any of the common C thread-safety issues.</p> <p>Usually, concurrency issues arise when two or more threads try to modify the same value in memory. In Python, this commonly occurs when a class or function defines native shared state, either via an attribute or a variable that can be modified from native code in each thread execution scope.</p> <p>The most common issues related to concurrency in the context of free-threaded CPython extensions are either dirty reads/writes to global/shared C state, unexpected behavior due to simultaneous access to C calls that are not thread-safe, and finally, major runtime crashes due to memory allocation issues and forbidden pointer lookups. While the first case depends on the actual implementation of the algorithm/routine and may produce unintended results, it would not cause a fatal crash of the interpreter, as opposed to the last two cases.</p> <p>In order to discover, handle and debug concurrency issues at large, there are several strategies, which we will summarize next.</p>"},{"location":"debugging/#pytest-plugins-to-discover-concurrency-issues","title":"pytest plugins to discover concurrency issues","text":"<p>As parallel testing has become a critical component to ensure compatibility with free-threaded CPython, several community-led pytest plugins have been implemented that attempt to smoke out issues by running all tests in a test suite in a concurrent manner:</p> <ul> <li>pytest-run-parallel</li> <li>pytest-freethreaded</li> </ul> <p>The advantage of using a pytest plugin as opposed to manually using the <code>threading</code> and/or <code>concurrent.futures</code> modules mainly resides in their ability to integrate with the ecosystem constructs like markers, fixtures, skip and failure flags. For more information regarding the usage of these libraries please refer to the documentation of each project.</p>"},{"location":"debugging/#repeated-test-execution","title":"Repeated test execution","text":"<p>Given the non-deterministic nature of parallel execution, tests for code that has a concurrency issue may still pass most of the time. In order to more reliably reproduce a test failure under concurrency, we recommend using pytest-repeat, which enables the <code>--count</code> flag in the <code>pytest</code> command:</p> <pre><code># Setting PYTHON_GIL=0 ensures that the GIL is effectively disabled.\nPYTHON_GIL=0 pytest -x -v --count=100 test_concurrent.py\n</code></pre> <p>We advise to set <code>count</code> to <code>100</code> (or even larger if needed), in order to ensure at least one concurrent clash event.</p>"},{"location":"debugging/#writing-explicitly-concurrent-test-cases","title":"Writing explicitly concurrent test cases","text":"<p>It may be desirable to have tests using, e.g., <code>threading</code> or <code>concurrent.futures</code> in your test suite in order to prevent adding additional test dependencies or to test a particular subset of tests for concurrency issues by default. The stdlib <code>threading</code> module defines several low-level parallel primitives that can be used to test for concurrency, while the <code>concurrent.futures</code> module provides higher-level constructs.</p> <p>For example, consider a method <code>MyClass.call_unsafe</code> that has been flagged as having concurrency issues since it mutates attributes of a shared object that is accessed by multiple threads. We can write a test for it using either <code>threading</code> or <code>concurrent.futures</code> primitives:</p> Example using threading: <pre><code>import threading\n\n# Library to test\nfrom mylib import MyClass\n\n\ndef test_call_unsafe_concurrent_threading():\n    # Defines a thread barrier that will be spawned before parallel execution\n    # this increases the probability of concurrent access clashes.\n    n_threads = 10\n    barrier = threading.Barrier(n_threads)\n\n    # This object will be shared by all the threads.\n    cls_instance = MyClass(...)\n\n    results = []\n\n    def closure():\n        # Ensure that all threads reach this point before concurrent execution.\n        barrier.wait()\n        r = cls_instance.call_unsafe()\n        results.append(r)\n\n    # Spawn n threads that call call_unsafe concurrently.\n    workers = []\n    for _ in range(0, n_threads):\n        workers.append(threading.Thread(target=closure))\n\n    for worker in workers:\n        worker.start()\n\n    for worker in workers:\n        worker.join()\n\n    # Do something about the results\n    assert check_results(results)\n</code></pre> Example using concurrent.futures: <pre><code>import threading\nfrom concurrent.futures import ThreadPoolExecutor\n\n# Library to test\nfrom mylib import MyClass\n\n\ndef test_call_unsafe_concurrent_pool():\n    # Defines a thread barrier that will be spawned before parallel execution\n    # this increases the probability of concurrent access clashes.\n    n_threads = 10\n    barrier = threading.Barrier(n_threads)\n\n    # This object will be shared by all the threads.\n    cls_instance = MyClass(...)\n\n    def closure():\n        # Ensure that all threads reach this point before concurrent execution.\n        barrier.wait()\n        r = cls_instance.call_unsafe()\n        return r\n\n    with ThreadPoolExecutor(max_workers=n_threads) as executor:\n        futures = [executor.submit(closure) for _ in range(n_threads)]\n\n    results = [f.result() for f in futures]\n\n    # Do something about the results\n    assert check_results(results)\n</code></pre>"},{"location":"debugging/#debugging-tests-that-depend-on-native-calls","title":"Debugging tests that depend on native calls","text":"<p>If your code has native dependencies, either via C/C++ or Cython, <code>gdb</code> (or <code>lldb</code>) can be used as follows:</p> <pre><code># Setting PYTHON_GIL=0 ensures that the GIL is effectively disabled.\nPYTHON_GIL=0 gdb --args python my_program.py --args ...\n\n# To test under pytest\nPYTHON_GIL=0 gdb --args python -m pytest -x -v \"test_here.py::TestClass::test_method\"\n\n# Using LLDB (under LLVM/clang)\nPYTHON_GIL=0 lldb -- $(which python) my_program.py\n\n# Using LLDB (and pyenv)\nPYTHON_GIL=0 lldb -- $(pyenv which python) $(pyenv which pytest) -x -v \"test_here.py::TestClass::test_method\"\n</code></pre> <p>When Python is run under <code>gdb</code>, several Python integration commands will be available, such commands start with the <code>py-</code> prefix. For instance, the <code>py-bt</code> allows to obtain a Python interpreter backtrace whenever the debugger hits a native frame, this allows to improve the tracking of execution between Python and native frames<sup>1</sup>.</p> <p>For more information about <code>gdb</code> and <code>lldb</code> commands, we encourage reading the GDB to LLDB command map page in the official LLVM docs.</p>"},{"location":"debugging/#cython-debugging","title":"Cython debugging","text":"<p>Since Cython produces intermediate C/C++ sources that then are compiled into native code, stepping through may get difficult if done solely from the C source file. In order to get through such difficulty, Cython includes the <code>cygdb</code> extension, which enables <code>gdb</code> to go through large sections of C code that are equivalent to a single Cython declaration.</p> <p>Enabling <code>cygdb</code> requires the compilation of Cython sources with the <code>--gdb</code> flag. After the sources are compiled and linked, it can be used as follows:</p> <pre><code># For example, running the tests of scikit-image.\n# build/cp313td/ contains the trace files generated by Cython to be compatible\n# with cygdb\nPYTHON_GIL=0 cygdb build/cp313td/ -- --args python -m  pytest -x -v skimage/\n</code></pre> <p>Since <code>cygdb</code> requires the Python interpreter version used to compile <code>gdb</code> to match the one to be used during the execution of the script, recompiling <code>gdb</code> will be necessary in order to ensure the most complete debugging experience. We recommend the <code>gdb</code> compilation instructions provided by the Linux from scratch project.</p> <p><code>cygdb</code> defines a set of commands prefixed by <code>cy</code> that replace the usual <code>gdb</code> commands. For example <code>cy run</code> will start the program with the Cython debugging extensions enabled, <code>cy break</code> will define a breakpoint on a function with the Cython definition name, <code>cy next</code> will step over a Cython line, which is equivalent to several lines in the produced C code.</p>"},{"location":"debugging/#detecting-issues-in-cpython","title":"Detecting issues in CPython","text":"<p>If a debugging session suggests that an error/bug is incoming from CPython, we recommend installing a debug instance. The easiest way to accomplish this is via <code>pyenv</code>:</p> <pre><code>pyenv install --debug --keep 3.13.1\n</code></pre> <p>This command will not only install a debug distribution of CPython, but also will ensure that the source files are kept as well, such files will be loaded by <code>gdb</code>/<code>lldb</code> at the moment of debugging. For more information regarding CPython installation sources, please visit the Installing a free-threaded Python page.</p>"},{"location":"debugging/#compiling-cpython-and-foundational-packages-with-threadsanitizer","title":"Compiling CPython and foundational packages with ThreadSanitizer","text":"<p>Thread sanitizer (or TSan) helps to detect C/C++ data races in concurrent systems. This tool can help to reveal free-threading related bugs in CPython and foundational packages (e.g. <code>numpy</code>). In this section we provide the commands to build a free-threading compatible CPython interpreter and packages with ThreadSanitizer and other hints to discover potential data races.</p>"},{"location":"debugging/#cpython_sanity-docker-images","title":"<code>cpython_sanity</code> docker images","text":"<p>To ease working with thread sanitizer in projects that use Python, NumPy, and SciPy, we have create a set of docker images that contain a pre-built Python interpreter and common dependencies that can be tricky to build.</p> <p>See the <code>cpython_sanity</code> repository for more information about how to use the docker images. Also see NumPy PR #28808, which adjusted NumPy TSAN CI to use the <code>ghcr.io/nascheme/numpy-tsan:3.14t-dev</code> docker image instead of building Python from source, saving ten minutes of compute time per CI run.</p>"},{"location":"debugging/#compile-free-threaded-cpython-with-threadsanitizer","title":"Compile free-threaded CPython with ThreadSanitizer","text":"<ul> <li>Clone the latest stable branch (<code>3.13</code>):</li> </ul> <pre><code>git clone https://github.com/python/cpython.git -b 3.13\n</code></pre> <ul> <li>Configure and build the interpreter. Below instructions are for Linux     (Windows and macOS may require some changes). We skip the instructions on how     to install the Clang compiler.</li> </ul> <pre><code>cd cpython\nCC=clang-18 CXX=clang++-18 ./configure --disable-gil --with-thread-sanitizer --prefix $PWD/cpython-tsan\nmake -j 8\nmake install\n</code></pre> <ul> <li>To use the built Python interpreter:</li> </ul> <pre><code># Create a virtual environment:\n$PWD/cpython-tsan/bin/python3.13t -m venv ~/tsanvenv\n# Then activate it:\nsource ~/tsanvenv/bin/activate\n\npython -VV\n# Python 3.13.1 experimental free-threading build (tags/v3.13.1:06714517797, Dec 19 2024, 10:06:54) [Clang 18.1.3 (1ubuntu1)]\nPYTHON_GIL=0 python -c \"import sys; print(sys._is_gil_enabled())\"\n# False\n\n# Exit the `cpython` folder (preparation for the next step below)\ncd ..\n</code></pre> <p>If you use pyenv, you can also enable a thread sanitizer build with <code>pyenv install</code> like so:</p> <pre><code>CC=/path/to/clang CXX=/path/to/clang++ CONFIGURE_OPTS=\"--with-thread-sanitizer\" pyenv install 3.14t-dev\n</code></pre> <p>And then activate the build with e.g. <code>pyenv local 3.14t-dev</code>.</p> <p>Note</p> <p>On MacOS, you may see messages like this when you start Python:</p> <pre><code>python(7027,0x1f6dfc240) malloc: nano zone abandoned due to inability to reserve vm space.\n</code></pre> <p>This message is being emitted by the MacOS malloc implementation. As explained here, this happens for any program compiled with ThreadSanitizer on MacOS and can be safely ignored by setting the <code>MallocNanoZone</code> environment variable to 0. You should only set this in session you are running ThreadSanitizer under, as this setting will slow down other programs that allocate memory.</p>"},{"location":"debugging/#compile-numpy-with-threadsanitizer","title":"Compile NumPy with ThreadSanitizer","text":"<ul> <li>Get the source code (for example, the <code>main</code> branch)</li> </ul> <pre><code>git clone --recursive https://github.com/numpy/numpy.git\n</code></pre> <ul> <li>Install the build requirements:</li> </ul> <pre><code>cd numpy\npython -m pip install -r requirements/build_requirements.txt\n</code></pre> <ul> <li>Build the package</li> </ul> <pre><code>CC=clang-18 CXX=clang++-18 python -m pip install -v . --no-build-isolation -Csetup-args=-Db_sanitize=thread\n# or with debug info\n# CC=clang-18 CXX=clang++-18 python -m pip install -v . --no-build-isolation -Csetup-args=-Db_sanitize=thread -Csetup-args=-Dbuildtype=debugoptimized\n</code></pre>"},{"location":"debugging/#running-python-under-threadsanitizer","title":"Running Python under ThreadSanitizer","text":""},{"location":"debugging/#useful-threadsanitizer-options","title":"Useful ThreadSanitizer options","text":"<ul> <li>By default ThreadSanitizer reports warnings. To stop execution on ThreadSanitizer errors, use:</li> </ul> <pre><code>TSAN_OPTIONS=halt_on_error=1 python -m pytest test.py\n</code></pre> <p>See the ThreadSanitizer documentation for a full listing of options accepted by ThreadSanitizer.</p> <ul> <li>To add ThreadSanitizer suppressions (written in a file: <code>tsan-suppressions</code>):</li> </ul> <pre><code># Let's show an example content of suppressions,\n# more info: https://github.com/google/sanitizers/wiki/ThreadSanitizerSuppressions\ncat $PWD/tsan-suppressions\n\nrace:llvm::RuntimeDyldELF::registerEHFrames\nrace:partial_vectorcall_fallback\nrace:dnnl_sgemm\n\n\nexport TSAN_OPTIONS=\"suppressions=$PWD/tsan-suppressions\" python -m pytest test.py\n</code></pre>"},{"location":"debugging/#running-pytest-tests-under-threadsanitizer","title":"Running pytest tests under ThreadSanitizer","text":"<p>By default, pytest captures all output from tests, this means that you might only see output like <code>ThreadSanitizer: reported 2 warnings</code>, but with no accompanying report with details about the warning.</p> <p>To ensure that pytest doesn't capture any output from ThreadSanitizer, you can pass <code>-s</code> (short for <code>--show-capture</code>) to your pytest invocation.</p> <p>Some authors of this guide have observed hangs running pytest with <code>halt_on_error=1</code>. If you observe hangs, try setting <code>halt_on_error=0</code> in TSAN_OPTIONS.</p> <p>The pytest-xdist plugin can also sometimes be problematic if a test runner happens to crash during execution. While <code>pytest-xdist</code> does have some support for detecting crashed worker, it is not foolproof and the authors of this guide have observed hangs on CI due to pytest-xdist not properly handling a worker failing due to a ThreadSanitizer error.</p> <p>The <code>pytest-xdist</code> plugin also makes it impossible to obtain stdout from a test runner, so there is no way to see ThreadSanitizer output if there is an issue. This can lead to hangs on CI machines with no accompanying error report to explain the nature of the hang. For that reason we suggest uninstalling <code>pytest-xdist</code> from your environment to ensure it isn't used. If you need to use <code>pytest-xdist</code> to make the tests complete in a reasonable amount of time, we suggest using <code>pytest-timeout</code> to ensure hung tests eventually exit, particularly on CI.</p> <p>ThreadSanitizer includes a check to ensure allocators never fail. This can lead to runtime crashes if a test happens to try allocating a very large block of memory specifically to ensure such an allocation does fail correctly. Set <code>allocator_may_return_null=1</code> in <code>TSAN_OPTIONS</code> to avoid this.</p> <p>If a ThreadSanitizer warning is detected, the exit code of the running process will be set to a nonzero value (66, by default). If for some reason that is problematic in your test suite then you can set <code>exitcode=0</code> in <code>TSAN_OPTIONS</code> to make ThreadSanitizer quit \"successfully\" if a warning is detected. For example, you might set this if a subprocess returning a nonzero exit code unexpectedly breaks a test.</p> <p>You might also find that running your test suite is very slow under ThreadSanitizer. Consider skipping tests that do not use threads, for example by only testing files that import <code>threading</code> or <code>concurrent.futures.ThreadPoolExecutor</code>. See this NumPy CI workflow that runs pytest on a subset of NumPy's tests. This will miss tests that spawn threads in native code (e.g. with OpenMP or other threading primitives) or use Python packages that spawn threads, but is a good option if your library doesn't do that.</p> <p>Altogether, a pytest invocation using ThreadSanitizer might look like:</p> <pre><code>$ TSAN_OPTIONS='allocator_may_return_null=1 halt_on_error=1' pytest -s\n</code></pre>"},{"location":"debugging/#using-addresssanitizer-to-detect-thread-safety-issues","title":"Using AddressSanitizer to detect thread safety issues","text":"<p>Since ThreadSanitizer adds significant overhead to both the Python interpreter and any native code compiled under ThreadSanitizer, many projects will be unable to run their full test suite under ThreadSanitizer in CI.</p> <p>For that reason, we also suggest setting up CI run for your full test suite using AddressSanitizer (or ASan). AddressSanitizer will detect if there are any memory safety issues triggered by multithreaded tests. While it will not detect data races that do not lead to observable memory safety issues, it will detect races that could lead to e.g. a segmentation fault and give precise debugging information about the nature of the memory safety issue. A developer could then look more closely at the issue using ThreadSanitizer outside of CI to more fully understand whether data races contributed to the memory safety issue.</p> <p>You can build Python with AddressSanitizer by passing <code>--with-address-sanitizer</code> to the CPython configure script. You can build NumPy with AddressSanitizer by passing <code>-Csetup-args=-Db_sanitize=address</code> as an argument to <code>pip install</code>.</p> <p>Like ThreadSanitizer, AddressSanitizer also accepts a number of options to control its behavior. See the documentation for more details. Note that both the CPython interpreter and many extensions have harmless memory leaks, so consider disabling the leak sanitizer built into AddressSanitizer by setting <code>ASAN_OPTIONS=\"detect_leaks=0\"</code>.</p> <ol> <li> <p>This feature is not correctly working on <code>lldb</code> after CPython 3.12.\u00a0\u21a9</p> </li> </ol>"},{"location":"dependencies/","title":"Handling dependencies that don\u2019t support free-threading","text":""},{"location":"dependencies/#build-dependencies-that-dont-support-free-threading","title":"Build dependencies that don't support free-threading","text":""},{"location":"dependencies/#cffi-does-not-yet-support-the-free-threaded-python-build","title":"CFFI does not yet support the free-threaded Python build","text":"<p>There is an open pull request adding support for the free-threaded build to CFFI. You can install CFFI from the pull request branch like this:</p> <pre><code>python -m pip install git+https://github.com/ngoldbaum/cffi-ft.git@cffi-ft\n</code></pre> <p>Additionally, you can declare a build-time dependency on the pull request branch using the following <code>pyproject.toml</code> snippet:</p> <pre><code>[build-system]\nrequires = [\n  \"cffi @ git+https://github.com/ngoldbaum/cffi-ft@cffi-ft\",\n]\n\n[project]\ndependencies = [\n  \"cffi @ git+https://github.com/ngoldbaum/cffi-ft@cffi-ft\",\n]\n</code></pre>"},{"location":"dependencies/#runtime-dependencies-that-dont-support-free-threading","title":"Runtime dependencies that don't support free-threading","text":""},{"location":"dependencies/#depending-on-pyyaml-use-pyyaml-ft","title":"Depending on PyYAML - use PyYAML-ft","text":"<p>If your library depends on PyYAML, you will need to take extra care to use it with free-threaded Python. PyYAML currently does not support free-threading and has some thread-safety issues. Its maintainers have decided to not port PyYAML before free-threading and Cython support for it have been more extensively tested.</p> <p>That's why we've created a fork of PyYAML with support for free-threading called PyYAML-ft. PyYAML users can switch to this fork if they want to test their code with the free-threaded build.</p> <p>Currently, PyYAML-ft only supports Python 3.13 and 3.13t (i.e. the free-threaded build of 3.13). To switch to it, you can add the following to your <code>requirements.txt</code>:</p> <pre><code>PyYAML; python_version &lt; '3.13'\nPyYAML-ft; python_version &gt;= '3.13'\n</code></pre> <p>If you define your dependencies in <code>pyproject.toml</code>, then you can do the following:</p> <pre><code>dependencies = [\n  \"PyYAML; python_version&lt;'3.13'\",\n  \"PyYAML-ft; python_version&gt;='3.13'\",\n]\n</code></pre>"},{"location":"dependencies/#different-module-name","title":"Different module name","text":"<p>PyYAML-ft uses a different module name (namely <code>yaml_ft</code>) than upstream PyYAML on purpose, so that both can be installed in an environment at the same time.</p> <p>If your library depends on both for different Python versions, you can do the following for ease of use:</p> <pre><code>try:\n    import yaml_ft as yaml\nexcept ModuleNotFoundError:\n    import yaml\n</code></pre>"},{"location":"faq/","title":"Frequently seen errors and how to fix them","text":"<p>These are error messages that we see come up often when working with code or development workflows that have not been updated to accommodate the free-threaded build. We also provide suggested fixes. Please send in pull requests to the repository for this document if you run into any confusing free-threading-specific errors that you suspect apply to other libraries and aren't covered here.</p>"},{"location":"faq/#cython-compilation-errors-unknown-type-name-__pyx_vectorcallfunc","title":"Cython compilation errors: <code>unknown type name '__pyx_vectorcallfunc'</code>","text":"<p>This happens if you try to build a Cython extension for the free-threaded build using a release of Cython that does not support it (&lt; 3.1.0). This might happen if a package pins an older version of Cython. You should check the build dependencies of the package and try relaxing any Cython pin or otherwise establish why the latest Cython release is not being installed at build time.</p> <p>See the porting guide for more details about porting Cython code to work under free-threading.</p>"},{"location":"faq/#what-does-runtimewarning-the-global-interpreter-lock-gil-has-been-enabled-mean","title":"What does <code>RuntimeWarning: The global interpreter lock (GIL) has been enabled</code> mean?","text":"<p>This happens when python imports a module defined in a native extension that does not explicitly declare support for running without the GIL. By default, native extensions do not support running without the GIL because of the long history of extensions assuming the GIL locks concurrent access to extension internals. As a precaution, when the free-threaded Python build imports such an extension, it assumes that the GIL is necessary to run the extension, and enables the GIL at runtime.</p> <p>If you have control over the code in the native extension, then you should update the extension to support the free-threaded build. See the guide section on that topic. If you do not control the extension or simply want to test running with the GIL disabled despite the extension not explicitly supporting it, then you can set either the <code>PYTHON_GIL</code> environment variable or the <code>-X gil</code> command-like flag for the python interpreter to <code>0</code> (i.e. the GIL is disabled). This skips the runtime check for whether extensions support running with the GIL disabled. See the section on running free-threaded Python for more details.</p>"},{"location":"faq/#pip-install-jupyter-fails","title":"<code>pip install jupyter</code> fails","text":"<p>This happens because some of the dependencies of the <code>jupyter</code> project on PyPI do not yet support the free-threaded build. In particular, <code>argon2-cffi</code> uses a C extension provided by the <code>argon2-cffi-bindings</code> project, which in turn uses CFFI to generate bindings for the <code>argon2</code> C password hashing library.</p> <p>Because CFFI does not yet ship a stable release that supports the free-threaded build, even if you have a compiler environment properly set up, the <code>argon2-cffi-bindings</code> build will fail.</p> <p>For now, we do not recommend trying to install jupyterlab into an environment managed by a free-threaded Python interpreter. Instead, we suggest installing a free-threaded Python kernel into a jupyterlab installation managed by a GIL-enabled python interpreter. See our instructions for installing a free-threaded Jupyter kernel for more details.</p> <p>That said, it is possible to install <code>jupyter</code> into a free-threaded Python environment and launch jupyterlab, see this issue for more details.</p>"},{"location":"faq/#what-does-py_limited_api-is-currently-incompatible-with-py_gil_disabled-mean","title":"What does \"Py_LIMITED_API is currently incompatible with Py_GIL_DISABLED\" mean?","text":"<p>You might see an error with this message generated by setuptools, but other tools might generate similar errors for the same reason.</p> <p>The ultimate problem is that CPython does not yet offer a stable ABI on the free-threaded build. See this CPython issue for more details. This means it's not yet possible to build an extension that any CPython version can import and instead libraries have to distribute one wheel for each version of free-threaded CPython. A future version of CPython will likely support a new version of the stable ABI that works correctly with both the GIL-enabled and free-threaded builds.</p> <p>Until then, tools or libraries that default to using the stable ABI via the limited API will generate build errors on the free-threaded build. The fix is usually to disable the <code>Py_LIMITED_API</code> declaration that triggers use of the stable ABI on the free-threaded build by using the following incantation:</p> <pre><code>import sysconfig\nFREETHREADED_BUILD = bool(sysconfig.get_config_var(\"Py_GIL_DISABLED\"))\n</code></pre> <p>If you spell it this way, <code>FREETHREADED_BUILD</code> will be <code>False</code> on the GIL-enabled build and <code>True</code> for free-threaded builds.</p>"},{"location":"installing-cpython/","title":"Installing Free-Threaded Python","text":"<p>To install a free-threaded CPython interpreter, you can choose from the following options:</p> <ul> <li>use a pre-built binary</li> <li>build from source</li> <li>use a container image</li> <li>install a Jupyter kernel</li> </ul> <p>To get started quickly, use a pre-built binary with python.org installers (nuget for Windows), linux distribution installers, or multi-platform package managers.</p> <p>Building from source is straightforward too. If you hit a bug that may involve CPython itself then you may want to build from source.</p>"},{"location":"installing-cpython/#use-a-pre-built-binary","title":"Use a pre-built binary","text":"<p>There are a growing number of options to install a free-threaded interpreter, including the python.org installers, Linux distro installers, and multi-platform package managers, like conda.</p> <p>Note</p> <p>When using these options, please check your <code>pip</code> version after the install succeeds. To check the version, run <code>python3.13t -m pip -V</code>.</p> <p>You should have a recent <code>pip</code> version (<code>&gt;=24.1</code>). Upgrade it if that isn't the case. Older <code>pip</code> versions will select incompatible wheels with the <code>cp313</code> tag (binary-incompatible) rather than the <code>cp313t</code> tag (compatible).</p> As a packager, what should I name the package and interpreter? <p>Please see this guidance from the Python Steering Council</p>"},{"location":"installing-cpython/#install-binaries-directly","title":"Install binaries directly","text":"<p>The python.org downloads page provides macOS and Windows installers that have experimental support.</p> <p>Currently, you must customize the install - e.g., for Windows there is a Download free-threaded binaries checkbox under \"Advanced Options\". See also the Using Python on Windows section of the Python 3.13 docs.</p> <p>Automating the process of downloading the official installers and installing the free-threaded binaries is also possible:</p> WindowsmacOS <p>Due to limitations of the Windows Python.org installer, using free-threaded Python installed from the Python.org installer may lead to trouble. In particular, if you install both a free-threaded and gil-enabled build of Python 3.13 using the Python.org installer, both installs will share a <code>site-packages</code> folder. This can very quickly lead to broken environments if packages for both versions are simultaneously installed.</p> <p>For that reason, we suggest using the <code>nuget</code> installer, which provides a separate <code>python-freethreaded</code> package that does not share an installation with the <code>python</code> package.</p> <pre><code>$url = 'https://www.nuget.org/api/v2/package/python-freethreaded/3.13.1'\nInvoke-WebRequest -Uri $url -OutFile 'python-freethreaded.3.13.1.nupkg'\nInstall-Package python-freethreaded -Scope CurrentUser -Source $pwd\n$python_dir = (Get-Item((Get-Package -Name python-freethreaded).Source)).DirectoryName\n$env:path = $python_dir + \"\\tools;\" + $python_dir + \"\\tools\\Scripts;\" + $env:Path\n</code></pre> <p>This will only modify your Path for the current Powershell session, so you will also need to permanently add the nuget package location to your Path to use it after closing the current session.</p> <p>If for some reason you need to use the Python.org installer, despite the problems described above, you can install it like so:</p> <pre><code>$url = 'https://www.python.org/ftp/python/3.13.1/python-3.13.1-amd64.exe'\nInvoke-WebRequest -Uri $url -OutFile 'python-3.13.1-amd64.exe'\n.\\python-3.13.1-amd64.exe /quiet Include_freethreaded=1\n</code></pre> <p>If you are running this script without administrator privileges, a UAC prompt will trigger when you try to run the installer. The resulting Python installation will be available afterwards in <code>AppData\\Local\\Programs\\Python\\Python313\\python3.13t.exe</code>. See Installing Without UI for more information.</p> <p>Install on a macOS laptop or desktop</p> <p>The official Python documentation's section on installing free-threaded binaries explains installation as well as any limitations of the free-threaded version. For convenience, we summarize the installation steps here:</p> <ol> <li> <p>Download the macOS installer package from python.org downloads page or the python.org pre-release downloads page.</p> </li> <li> <p>Run the installer.</p> </li> <li> <p>On the Installation Type screen (\"Standard Installation on Macintosh HD\"), click the \"Customize\" button.</p> </li> <li> <p>On the \"Customize Install on Macintosh HD\" screen, check the \" Free-threaded Python [experimental]\" option, and click \"install\".</p> </li> </ol> <p>Advanced installation (CI)</p> <p>This process installs the free-threaded version of Python 3.13.3 using the command line for more complex cases, such as running CI. It follows a similar process described in the CPython documentation for installing a binary using the command line.</p> <p>While this process installs the free-threaded version of Python 3.13.3, you can install other versions by substituting the version number in the following steps.</p> <ol> <li> <p>Download the installer package from python.org.</p> <pre><code>curl -O https://www.python.org/ftp/python/3.13.3/python-3.13.3-macos11.pkg\n</code></pre> </li> <li> <p>Create a <code>choicechanges.plist</code> file to customize the install to enable the PythonTFramework-3.13 package and accept the other defaults (install all other packages).</p> <pre><code>cat &gt; ./choicechanges.plist &lt;&lt;EOF\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"&gt;\n&lt;plist version=\"1.0\"&gt;\n&lt;array&gt;\n        &lt;dict&gt;\n                &lt;key&gt;attributeSetting&lt;/key&gt;\n                &lt;integer&gt;1&lt;/integer&gt;\n                &lt;key&gt;choiceAttribute&lt;/key&gt;\n                &lt;string&gt;selected&lt;/string&gt;\n                &lt;key&gt;choiceIdentifier&lt;/key&gt;\n                &lt;string&gt;org.python.Python.PythonTFramework-3.13&lt;/string&gt;\n        &lt;/dict&gt;\n&lt;/array&gt;\n&lt;/plist&gt;\nEOF\n</code></pre> </li> <li> <p>Run the installer.</p> <pre><code>sudo installer -pkg ./python-3.13.3-macos11.pkg \\\n    -applyChoiceChangesXML ./choicechanges.plist \\\n    -target /\n</code></pre> </li> <li> <p>Remove the package installer.</p> <pre><code>rm -f python-3.13.3-macos11.pkg\n</code></pre> </li> </ol> <p>See also this Github issue for more information.</p>"},{"location":"installing-cpython/#linux-distribution-installers","title":"Linux distribution installers","text":"FedoraNixpkgsUbuntu <p>Fedora ships a packaged version, which you can install with:</p> <pre><code>sudo dnf install python3.13-freethreading\n</code></pre> <p>This will install the interpreter at <code>/usr/bin/python3.13t</code>.</p> <p>Nixpkgs provides cached builds under the <code>python313FreeThreading</code> attribute from NixOS 24.05 and newer.</p> <p>With <code>flakes</code> enabled the following command will drop you in an ephemeral shell:</p> <pre><code>nix shell nixpkgs#python313FreeThreading\n</code></pre> <p>Without <code>flakes</code>, make sure to update your nixpkgs channel first:</p> <pre><code>sudo nix-channel --update\nnix-shell -p python313FreeThreading\n</code></pre> <p>For Ubuntu you can use the deadsnakes PPA by adding it to your repositories and then installing <code>python3.13-nogil</code>:</p> <pre><code>sudo add-apt-repository ppa:deadsnakes\nsudo apt-get update\nsudo apt-get install python3.13-nogil\n</code></pre>"},{"location":"installing-cpython/#multi-platform-package-managers","title":"Multi-platform Package Managers","text":"Conda-forgeAnaconda TestingHomebrewuv <pre><code>mamba create -n nogil -c conda-forge python-freethreading\n</code></pre> <p>or with conda:</p> <pre><code>conda create -n nogil --override-channels -c conda-forge python-freethreading\n</code></pre> <p>Anaconda's test channel includes the Python interpreter and ABI-compatible builds of many common packages, like NumPy, Cython, Pandas, etc. These packages use the <code>python_abi</code> metapackage and should be compatible with conda-forge:</p> <pre><code>conda create -n nogil --override-channels -c ad-testing/label/py313 -c https://repo.anaconda.com/pkgs/main python-freethreading\n</code></pre> <p>Full list of Anaconda test packages built with free-threading ABI.</p> <p>On macOS and Linux, you can use Homebrew:</p> <pre><code>brew install python-freethreading\n</code></pre> <p>This will install the interpreter at <code>$(brew --prefix)/bin/python3.13t</code>.</p> <p>On macOS, the Python framework built with the free-threading ABI can be found at <code>$(brew --prefix)/Frameworks/PythonT.framework</code>.</p> <p>If you have uv installed, you can create a virtual environment using free-threaded Python by specifying \"3.13t\" as the python version:</p> <pre><code>uv venv --python 3.13t\n</code></pre>"},{"location":"installing-cpython/#build-from-source","title":"Build from source","text":"<p>Currently we suggest building CPython from source using the latest version of the CPython <code>main</code> branch. See the build instructions in the CPython developer guide. You will need to install needed third-party dependencies before building. To build the free-threaded version of CPython, pass <code>--disable-gil</code> to the <code>configure</code> script:</p> <pre><code>./configure --with-pydebug --disable-gil\n</code></pre> <p>If you will be switching Python versions often, it may make sense to build CPython using pyenv. In order to do that, you can use the following:</p> <pre><code>pyenv install --debug --keep 3.13.1\n</code></pre>"},{"location":"installing-cpython/#use-containers","title":"Use containers","text":"<p>The manylinux containers have free-threaded builds. You can use any of the actively supported images:</p> <ul> <li><code>quay.io/pypa/manylinux_2_28_...</code></li> <li><code>quay.io/pypa/musllinux_1_1_...</code></li> <li><code>quay.io/pypa/musllinux_1_2_...</code></li> <li><code>quay.io/pypa/manylinux2014_...</code></li> </ul> <p>Replace <code>...</code> with your desired architecture, such as <code>x86_64</code> or <code>aarch64</code>.</p> <p>These images have <code>python3.13t</code> available, along with other commonly used tools that can target it like the latest <code>pip</code>, <code>pipx</code>, and <code>uv</code>.</p>"},{"location":"installing-cpython/#installing-a-free-threaded-jupyter-kernel","title":"Installing a free-threaded Jupyter kernel","text":"<p>While Jupyter does not currently support free-threaded Python, you can run Jupyter with a regular build of Python and a free-threaded Jupyter kernel.</p>"},{"location":"installing-cpython/#launch-using-regular-python-and-free-threaded-jupyter-kernel","title":"Launch using regular Python and free-threaded Jupyter kernel","text":"<p>Install the free-threaded Jupyter kernel to a location that is visible to both Python installations:</p> <pre><code>python3.13t -m ipykernel install --name python3.13t --user\n</code></pre> <p>You should be able to launch new jupyterlab or jupyter notebook sessions using the <code>python3.13t</code> kernel to experiment with free-threaded Python.</p>"},{"location":"installing-cpython/#launch-using-free-threaded-python-and-free-threaded-jupyter-kernel","title":"Launch using free-threaded Python and free-threaded Jupyter kernel","text":"<p>It is also possible to launch jupyterlab on the free-threaded build, see this issue comment for more details.</p>"},{"location":"porting-extensions/","title":"Updating Extension Modules","text":"<p>Here we are going to re-hash some of the same topics covered in the previous section but with a focus on advice for updating native extension modules, particularly modules relying directly on the CPython C API. The general advice remains the same: identify supported multithreaded workflows, add testing, and fix and identified thread safety issues. We will also describe how to handle some common thread-unsafe patterns we have found in many extension modules across the open source ecosystem.</p>"},{"location":"porting-extensions/#declaring-free-threaded-support","title":"Declaring free-threaded support","text":"<p>Extension modules need to explicitly indicate they support running with the GIL disabled, otherwise a warning is printed and the GIL is re-enabled at runtime after importing a module that does not support the GIL.</p> C APICythonPybind11nanobindPyO3f2py <p>C or C++ extension modules using multi-phase initialization can specify the <code>Py_mod_gil</code> module slot like this:</p> <pre><code>static PyModuleDef_Slot module_slots[] = {\n    ...\n#ifdef Py_GIL_DISABLED\n    {Py_mod_gil, Py_MOD_GIL_NOT_USED},\n#endif\n    {0, NULL}\n};\n</code></pre> <p>The <code>Py_mod_gil</code> slot has no effect in the non-free-threaded build.</p> <p>Extensions that use single-phase initialization need to call <code>PyUnstable_Module_SetGIL()</code> in the module's initialization function:</p> <pre><code>PyMODINIT_FUNC\nPyInit__module(void)\n{\n    PyObject *mod = PyModule_Create(&amp;module);\n    if (mod == NULL) {\n        return NULL;\n    }\n\n#ifdef Py_GIL_DISABLED\n    PyUnstable_Module_SetGIL(mod, Py_MOD_GIL_NOT_USED);\n#endif\n\n    return mod;\n}\n</code></pre> <p>See the Free threading section in the Cython user guide for detailed recommendations. Keep in mind that support for free-threaded Python in Cython code is currently considered experimental, so suggestions are subject to change.</p> <p>Starting with Cython 3.1.0 extension modules written in Cython can declare free-threaded support using the <code>freethreading_compatible</code> compiler directive.</p> <p>You can do this in one of several ways, e.g., in a source file:</p> <pre><code># cython: freethreading_compatible=True\n</code></pre> <p>Or by passing the directive when invoking the <code>cython</code> executable:</p> <pre><code>$ cython -X freethreading_compatible=True\n</code></pre> <p>Or via a build system specific way of passing directives to Cython.</p> <p>Tip</p> <p>Here are a few examples of how to globally enable the directive in a few popular build systems:</p> setuptoolsMeson <p>When using setuptools, you can pass the <code>compiler_directives</code> keyword argument to <code>cythonize</code>:</p> <pre><code>from Cython.Compiler.Version import version as cython_version\nfrom packaging.version import Version\n\ncompiler_directives = {}\nif Version(cython_version) &gt;= Version(\"3.1.0\"):\n    compiler_directives[\"freethreading_compatible\"] = True\n\nsetup(\n    ext_modules=cythonize(\n        extensions,\n        compiler_directives=compiler_directives,\n    )\n)\n</code></pre> <p>When using Meson, you can add the directive to the <code>cython_args</code> you're passing to <code>py.extension_module</code>:</p> <pre><code>cy = meson.get_compiler('cython')\n\ncython_args = []\nif cy.version().version_compare('&gt;=3.1.0')\n    cython_args += ['-Xfreethreading_compatible=True']\nendif\n\npy.extension_module('modulename'\n    'source.pyx',\n    cython_args: cython_args,\n    ...\n)\n</code></pre> <p>You can also globally add the directive for all Cython extension modules:</p> <pre><code>cy = meson.get_compiler('cython')\nif cy.version().version_compare('&gt;=3.1.0')\n    add_project_arguments('-Xfreethreading_compatible=true', language : 'cython')\nendif\n</code></pre> <p>C++ extension modules making use of <code>pybind11</code> can easily declare support for running with the GIL disabled via the <code>gil_not_used</code> argument to <code>create_extension_module</code>. Example:</p> <pre><code>#include &lt;pybind11/pybind11.h&gt;\nnamespace py = pybind11;\n\nPYBIND11_MODULE(example, m, py::mod_gil_not_used()) {\n    ...\n}\n</code></pre> <p>C++ extension modules making use of <code>nanobind</code> can declare support for running with the GIL disabled by passing the <code>FREE_THREADED</code> argument to the <code>nanobind_add_module</code> CMake target command. Example:</p> <pre><code>nanobind_add_module(\n  my_ext                   # Target name\n  FREE_THREADED            # Opt into free-threading\n  my_ext.h                 # Source code files below\n  my_ext.cpp)\n</code></pre> <p>If you use the CPython C API in Rust via PyO3, then you can follow the PyO3 Guide section on supporting free-threaded Python. You must also update your extension to at least version 0.23.</p> <p>You should write multithreaded tests of any code you expose to Python. See our guide on updating test suites for more details. You should fix any thread safety issues you discover while running multithreaded tests.</p> <p>As of PyO3 0.23, PyO3 enforces Rust's borrow checking rules at runtime and may produce runtime panics if you simultaneously mutably borrow data in more than one thread. You may want to consider storing state in using atomic data structures, with mutexes or locks, or behind <code>Arc</code> pointers.</p> <p>Once you are satisfied the Python modules defined by your rust crate are thread safe, you can pass <code>gil_used = false</code> to the <code>pymodule</code> macro:</p> <pre><code>#[pymodule(gil_used = false)]\nfn my_module(py: Python, m: &amp;Bound&lt;'_, PyModule&gt;) -&gt; PyResult&lt;()&gt; {\n    ...\n}\n</code></pre> <p>If you define any modules procedurally by manually creating a <code>PyModule</code> struct without using the <code>pymodule</code> macro, you can call <code>PyModuleMethods::gil_used</code> after instantiating the module.</p> <p>If you use the <code>pyo3-ffi</code> crate and/or <code>unsafe</code> FFI calls to call directly into the C API, then see the section on porting C extensions in this guide as well as the PyO3 source code.</p> <p>Starting with NumPy 2.1.0, extension modules containing f2py-wrapped Fortran code can declare they are thread-safe and support free-threading using the <code>--freethreading-compatible</code> command-line argument:</p> <pre><code>$ python -m numpy.f2py -c code.f -m my_module --freethreading-compatible\n</code></pre> <p>If you publish binaries and have downstream libraries that depend on your library, we suggest adding support as described above and uploading nightly wheels as soon as basic support for the free-threaded build is established in the development branch. This will ease the work of libraries that depend on yours to also add support for the free-threaded build.</p>"},{"location":"porting-extensions/#working-with-the-free-threaded-cpython-interpreter-runtime","title":"Working with the free-threaded CPython interpreter runtime","text":"<p>Many people are surprised to learn that almost all native extensions written with the GIL-enabled build in mind compile and run with minimal changes on the free-threaded build. They often ask questions like, \"if there is no GIL, doesn't that mean there's no need to call e.g. <code>PyGilState_Ensure()</code> before calling into the C API and no need to call <code>Py_BEGIN_ALLOW_THREADS</code> to release the GIL before doing I/O or a long-running computation?\". Bindings generators like Cython, PyO3, or Pybind11 all also have syntax for explicitly acquiring and releasing the GIL. Won't all this code need to change?</p> <p>The answer is no. To understand, let's first take a look at the diagram below, which illustrates a snapshot of the state of a multithreaded Python application that has native extensions.</p> <p></p> <p>A diagramatic snapshot of the state of a multithreaded Python application running on the GIL-enabled interpreter</p> <p>In this diagram, each thread spindle symbol represents a thread that is running code inside a native extension. The lock icon indicates whether the thread holds the GIL - only one thread can acquire the GIL at a time, indicated by the fastened lock on the thread calling into the CPython C API. The bottom row of symbols indicates what work each thread is doing. You can see that even with the GIL it is possible to get multithreaded parallelism, so long as there are threads that do not have the GIL acquired and are not waiting to acquire the GIL. Usually, this means a thread is doing I/O or a long-running calculation that does not need any state or functionality from the CPython runtime.</p> <p>In addition to the lock icon indicating whether the GIL is acquired, each thread icon is either plugged in or unplugged from the interpreter runtime, indicating it has an attached or detached thread state. In the GIL-enabled build, only one thread can have an attached thread state and hold the GIL, while all other unplugged threads wait for the GIL to be released.</p> <p>In the free-threaded build the GIL is disabled but threads can still either have attached or detached thread states. As in the GIL-enabled build, only attached threads can use interpreter state but, because there is no GIL, many threads can simultaneously call into the CPython C API.</p> <p>The state of a running free-threaded application is illustrated in the diagram below.</p> <p></p> <p>A diagramatic snapshot of the state of a multithreaded Python application running on the free-threaded-enabled interpreter</p> <p>In the free-threaded build, the GIL is disabled, so this diagram doesn't have lock icons. Because there is no GIL, threads do not need to wait to acquire it, and multiple threads can simultaneously call into the CPython C API.</p> <p>The icons indicating whether threads are attached or detached are still present. As discussed above, this is because it is still necessary to explicitly attach and detach from the runtime in the free-threaded build, despite the fact that there isn't a GIL.</p> <p>You might wonder why it's still necessary to detach from the runtime when doing I/O or a long-running native calculation. This is because there are still times when the interpreter needs to globally synchronize the state of all threads. For example, the free-threaded interpreter uses a stop-the-world garbage collection scheme, which requires all threads to be detached from the runtime before it can start. If you don't explicitly detach before doing a long-running operation that does not require the runtime, the interpreter may be blocked on running the garbage collector or doing any other operation that requires a globally consistent view of all threads.</p> <p></p> <p>Attaching and detaching from the runtime uses the same code as in the GIL-enabled build</p> <p>As illustrated above, attaching and detaching from the runtime uses exactly the same code in the free-threaded build as is used in the GIL-enabled build to acquire and release the GIL. It is an unfortunate naming issue that <code>PyGILState_Ensure</code> and <code>PyGILState_Release</code> has \"<code>GIL</code>\" in the name of the function, despite the lack of a GIL on the free-threaded build. It's likely that the C API in future Python version will fix this naming issue.</p> <p>Hopefully you now have a better mental model for how native code interacts with the CPython interpreter runtime in the free-threaded build and how it is similar to what happens on the GIL-enabled build, and what exactly it means for multiple threads to simultaneously execute Python code.</p> <p>You might also see how extension modules written assuming that Python can call into the extension in one thread at a time might lead to problematic thread-unsafe behavior. Doubly so now that more than one Python thread can simultaneously access any state stored in the extension in the free-threaded build.</p>"},{"location":"porting-extensions/#porting-c-extensions","title":"Porting C Extensions","text":"<p>The CPython C API exposes the <code>Py_GIL_DISABLED</code> macro in the free-threaded build. You can use it to enable low-level code that only runs under the free-threaded build, isolating possibly performance-impacting changes to the free-threaded build:</p> <pre><code>#ifdef Py_GIL_DISABLED\n// free-threaded specific code goes here\n#endif\n\n#ifndef Py_GIL_DISABLED\n// code for gil-enabled builds goes here\n#endif\n</code></pre>"},{"location":"porting-extensions/#locking-and-synchronization-primitives","title":"Locking and Synchronization Primitives","text":""},{"location":"porting-extensions/#native-mutexes","title":"Native mutexes","text":"<p>If your extension is written in C++, Rust, or another modern language that exposes locking primitives in the standard library, you should consider using the locking primitives provided by your language or framework to add locks when needed.</p> <p>If you need to call arbitrary Python code while the lock is held, care should be taken to avoid creating deadlocks with the GIL on the GIL-enabled build.</p>"},{"location":"porting-extensions/#pymutex","title":"<code>PyMutex</code>","text":"<p>For C code or C-like C++ code, the CPython 3.13 C API exposes <code>PyMutex</code>, a high-performance locking primitive that supports static allocation. As of CPython 3.13, the mutex requires only one byte for storage, but future versions of CPython may change that, so you should not rely on the size of <code>PyMutex</code> in your code.</p> <p>You can use <code>PyMutex</code> in both the free-threaded and GIL-enabled build of Python 3.13 or newer. <code>PyMutex</code> is hooked into the CPython runtime, so that if a thread tries to acquire the mutex and ends up blocked, garbage collection can still proceed and, in the GIL-enabled build, the blocked thread releases the GIL, allowing other threads to continue running. This implies that it is impossible to create a deadlock between a <code>PyMutex</code> and the GIL. For this reason, it is not necessary to add code for the GIL-enabled build to ensure the GIL is released before acquiring a <code>PyMutex</code>. If you do not call into the CPython C API while holding the lock, <code>PyMutex</code> has no special advantages over other mutexes, besides low-level details like performance or the size of the mutex object in memory.</p> <p>See the section on dealing with thread-unsafe low-level libraries below for an example using PyMutex to lock around a thread-unsafe C library.</p>"},{"location":"porting-extensions/#critical-sections","title":"Critical Sections","text":"<p>Python 3.13 or newer also offers a critical section API that is useful for locking either a single object or a pair of objects during a low-level operation. The critical section API is intended to provide weaker, but still useful locking guarantees compared to directly locking access to an object using a mutex. This provides similar guarantees to the GIL and avoids the risk of deadlocks introduced by locking individual objects.</p> <p>The main difference compared with using a per-object lock is that active critical sections are suspended if a thread calls <code>PyEval_SaveThread</code> (e.g. when the GIL is released on the GIL-enabled build), and then restored when the thread calls <code>PyEval_RestoreThread</code> (e.g. when the GIL is re-acquired on the GIL-enabled build). This means that while the critical sections are suspended, it's possible for any thread to re-acquire a thread state and mutate the locked object. This can also happen with the GIL, since the GIL is a re-entrant lock, and extensions are allowed to recursively release and acquire it in an interleaved manner.</p> <p>Critical sections are most useful when implementing the low-level internals of a custom object that you fully control. You can apply critical sections around modification of internal state to effectively serialize access to that state.</p> <p>See the section below on dealing with thread-unsafe objects for an example using the critical section API.</p>"},{"location":"porting-extensions/#dealing-with-global-state","title":"Dealing with global state","text":"<p>Many CPython C extensions make strong assumptions about the GIL. For example, before NumPy 2.1.0, the C code in NumPy made extensive use of C static global variables for storing settings, state, and caches. With the GIL, it is possible for Python threads to produce non-deterministic results from a calculation, but it is not possible for two C threads to simultaneously see the state of the C global variables, so no data races are possible.</p> <p>In free-threaded Python, global state like this is no longer safe against data races and undefined behavior in C code. A cache of <code>PyObject</code> pointers stored in a C global array can be overwritten simultaneously by multiple Python threads, leading to memory corruption and segfaults.</p>"},{"location":"porting-extensions/#converting-global-state-to-thread-local-state","title":"Converting global state to thread local-state","text":"<p>Often the easiest way to fix data races due to global state is to convert the global state to thread local state.</p> <p>Python and Cython code can make use of <code>threading.local</code> to declare a thread-local Python object. C and C++ code can also use the <code>Py_tss API</code> to store thread-local Python object references. PEP 539 has more details about the <code>Py_tss</code> API.</p> <p>Low-level C or C++ code can make use of the <code>thread_local</code> storage specified by recent standard versions. Note that standardization of thread-local storage in C has been slower than C++, so you may need to use platform-specific definitions to declare variables with thread-local storage. Also note that thread-local storage on MSVC has caveats, and you should not use thread-local storage for anything besides statically defined integers and pointers.</p> <p>NumPy has a <code>NPY_TLS</code> macro in the <code>numpy/npy_common.h</code> header. While you can include the numpy header and use <code>NPY_TLS</code> directly on NumPy 2.1 or newer, you can also add the definition to your own codebase, along with some build configuration tests to test for the correct definition to use.</p>"},{"location":"porting-extensions/#making-global-caches-thread-safe","title":"Making global caches thread-safe","text":"<p>Global caches are also a common source of thread safety issues. For example, if a function requires an expensive intermediate result that only needs to be calculated once, many C extensions store the result in a global variable. This can lead to data races and memory corruption if more than one thread simultaneously tries to fill the cache.</p> <p>If the cache is not critical for performance, consider simply disabling the cache in the free-threaded build:</p> <pre><code>static int *cache = NULL;\n\nint my_function_with_a_cache(void) {\n    int *my_cache = NULL;\n#ifndef Py_GIL_DISABLED\n    if (cache == NULL) {\n        cache = get_expensive_result();\n    }\n    my_cache = cache;\n#else\n    my_cache = get_expensive_result();\n#endif;\n    // use the cache\n}\n</code></pre> <p>CPython holds a per-module lock during import. This lock can be released to avoid deadlocks in unusual cases, but in most situations module initialization happens exactly once per interpreter in one C thread. Modules using static single-phase initialization can therefore set up per-module state in the <code>PyInit</code> function without worrying about concurrent initialization of modules in different threads. For example, you might set up a global static cache that is read-only after module initialization like this:</p> <pre><code>static int *cache = NULL;\n\nPyMODINIT_FUNC\nPyInit__module(void)\n{\n    PyObject *mod = PyModule_Create(&amp;module);\n    if (mod == NULL) {\n        return NULL;\n    }\n\n    // don't need to lock or do anything special\n    cache = setup_cache();\n\n    // do rest of initialization\n}\n</code></pre> <p>You can then read from <code>cache</code> at runtime in a context where you know the module is initialized without worrying about whether or not the per-module static cache is initialized.</p> <p>If the cache is critical for performance, cannot be generated at import time, but generally gets filled quickly after a program begins, then you will need to use a single-initialization API to ensure the cache is only ever initialized once. In C++, use <code>std::once_flag</code> or <code>std::call_once</code>.</p> <p>C does not have an equivalent portable API for single initialization. If you need that, take a look at this NumPy PR for an example using atomic operations and a global mutex.</p> <p>If the cache is in the form of a data container, then you can lock access to the container, like in the following example:</p> <pre><code>#ifdef Py_GIL_DISABLED\nstatic PyMutex cache_lock = {0};\n#define LOCK() PyMutex_Lock(&amp;cache_lock)\n#define UNLOCK() PyMutex_Unlock(&amp;cache_lock)\n#else\n#define LOCK()\n#define UNLOCK()\n#endif\n\nstatic int *cache = NULL;\nstatic PyObject *global_table = NULL;\n\nint initialize_table(void) {\n    // called during module initialization\n    global_table = PyDict_New();\n    return;\n}\n\nint function_accessing_the_cache(void) {\n    LOCK();\n    // use the cache\n\n    UNLOCK();\n}\n</code></pre> <p>Note</p> <p>Note that, while the NumPy PR linked above uses <code>PyThread_type_lock</code>, that is only because <code>PyMutex</code> was not part of the public Python C API at the time. We recommend always using <code>PyMutex</code>. For pointers on how to do that, check this NumPy PR that ports all <code>PyThread_type_lock</code> usages to <code>PyMutex</code>.</p>"},{"location":"porting-extensions/#dealing-with-thread-unsafe-native-libraries","title":"Dealing with thread-unsafe native libraries","text":"<p>Many C, C++, and Fortran libraries are not written in a thread-safe manner. It is still possible to call these libraries from free-threaded Python, but wrappers must add appropriate locks to prevent undefined behavior.</p> <p>There are two kinds of thread unsafe libraries: reentrant and non-reentrant. A reentrant library generally will expose state as a struct that must be passed to library functions. So long as the state struct is not shared between threads, functions in the library can be safely executed simultaneously.</p> <p>Wrapping reentrant libraries requires adding locking whenever the state struct is accessed.</p> <pre><code>typedef struct lib_state_struct {\n    low_level_library_state *state;\n    PyMutex lock;\n} lib_state_struct;\n\nint call_library_function(lib_state_struct *lib_state) {\n    PyMutex_Lock(&amp;lib_state-&gt;lock);\n    library_function(lib_state-&gt;state);\n    PyMutex_Unlock(&amp;lib_state-&gt;lock)\n}\n\nint call_another_library_function(lib_state_struct *lib_state) {\n    PyMutex_Lock(&amp;lib_state-&gt;lock);\n    another_library_function(lib_state-&gt;state);\n    PyMutex_Unlock(&amp;lib_state-&gt;lock)\n}\n</code></pre> <p>With this setup, if two threads call <code>library_function</code> and <code>another_library_functions</code> simultaneously, one thread will block until the other thread finishes, preventing concurrent access to <code>lib_state-&gt;state</code>.</p> <p>Non-reentrant libraries provide an even weaker guarantee: threads cannot call library functions simultaneously without causing undefined behavior. Generally this is due to use of global static state in the library. This means that non-reentrant libraries require a global lock:</p> <pre><code>static PyMutex global_lock = {0};\n\nint call_library_function(int *argument) {\n    PyMutex_Lock(&amp;global_lock);\n    library_function(argument);\n    PyMutex_Unlock(&amp;global_lock);\n}\n</code></pre> <p>Any other wrapped function needs similar locking around each call into the library.</p>"},{"location":"porting-extensions/#dealing-with-thread-unsafe-objects","title":"Dealing with thread-unsafe objects","text":"<p>Similar to the section above, objects may need locking or atomics if they can be concurrently modified from multiple threads. CPython 3.13 exposes a public C API that allows users to use the built-in per-object locks.</p> <p>For example the following code:</p> <pre><code>int do_modification(MyObject *obj) {\n    return modification_on_obj(obj);\n}\n</code></pre> <p>Should be transformed to:</p> <pre><code>int do_modification(MyObject *obj) {\n    int res;\n    Py_BEGIN_CRITICAL_SECTION(obj);\n    res = modification_on_obj(obj);\n    Py_END_CRITICAL_SECTION(obj);\n    return res;\n}\n</code></pre> <p>A variant for locking two objects at once is also available. For more information about <code>Py_BEGIN_CRITICAL_SECTION</code>, please see the Python C API documentation on critical sections.</p>"},{"location":"porting-extensions/#cython-thread-safety","title":"Cython thread safety","text":"<p>See the free-threading section in the Cython user guide for recommendations from the Cython developers. In particular, see the recommendations about thread safety and the opinionated suggestions for how to deal with thread safety in Cython extensions. At the time of writing, Cython does not automatically ensure any significant level of thread safety, so it is up to the author of a Cython extension to add locking or make use of critical sections as needed to ensure thread safety.</p> <p>It is normal for an extension to build without modification using Cython 3.1.0 or newer, but keep in mind that building or running without crashing does not imply that code will also be thread-safe and deterministic when used in a multithreaded context.</p>"},{"location":"porting-extensions/#cpython-c-api-usage","title":"CPython C API usage","text":"<p>In the free-threaded build it is possible for the reference count of an object to change \"underneath\" a running thread when it is mutated by another thread. This means that many APIs that assume reference counts cannot be updated by another thread while it is running are no longer thread-safe. In particular, C code returning \"borrowed\" references to Python objects in mutable containers like lists may introduce thread safety issues. A borrowed reference happens when a C API function does not increment the reference count of a Python object before returning the object to the caller. \"New\" references are safe to use until the owning thread releases the reference, as in non free-threaded code.</p> <p>Most direct uses of the CPython C API are thread-safe. There is no need to add locking for scenarios that should be bugs in CPython. You can assume, for example, that the initializer for a Python object can only be called by one thread and the C-level implementation of a Python function can only be called on one thread. Accessing the arguments of a Python function is thread-safe no matter what C API constructs are used and no matter whether the reference is borrowed or owned because two threads can't simultaneously call the same function with the same arguments from the same Python-level context. Of course it's possible to implement argument parsing in a thread-unsafe manner using thread-unsafe C or C++ constructs, but it's not possible to do so using the CPython C API.</p>"},{"location":"porting-extensions/#unsafe-apis-returning-borrowed-references","title":"Unsafe APIs returning borrowed references","text":"<p>The <code>PyDict</code> and <code>PyList</code> APIs contain many functions returning borrowed references to items in dicts and lists. Since these containers are mutable, it's possible for another thread to delete the item from the container, leading to the item being de-allocated while the borrowed reference is still \"alive\". Even code like this:</p> <pre><code>PyObject *item = Py_NewRef(PyList_GetItem(list_object, 0))\n</code></pre> <p>Is not thread-safe, because in principle it's possible for the list item to be de-allocated before <code>Py_NewRef</code> gets a chance to increment the reference count.</p> <p>For that reason, you should inspect Python C API code to look for patterns where a borrowed reference is returned to a shared, mutable data structure, and replace uses of APIs like <code>PyList_GetItem</code> with APIs exposed by the CPython C API returning strong references like <code>PyList_GetItemRef</code>. Not all usages are problematic (see above) and we do not currently suggest converting all usages of possibly unsafe APIs returning borrowed references to return new reference. This would introduce unnecessary reference count churn in situations that are thread-safe by construction and also likely introduce new reference counting bugs in C or C++ code using the C API directly. However, many usages are unsafe, and maintaining a borrowed reference to an objects that could be exposed to another thread is unsafe.</p> <p>A good starting place to find instances of this would be to look for usages of the unsafe borrowed reference APIs mentioned in the free-threading compatibility docs.</p>"},{"location":"porting-extensions/#adopt-pythoncapi-compat-to-use-new-c-api-functions","title":"Adopt <code>pythoncapi-compat</code> to use new C API functions","text":"<p>Rather than maintaining compatibility shims to use functions added to the C API for Python 3.13 like <code>PyList_GetItemRef</code> while maintaining compatibility with earlier Python versions, we suggest adopting the <code>pythoncapi-compat</code> project as a build-time dependency. This is a header-only library that can be vendored as e.g. a git submodule and included to expose shims for C API functions on older versions of Python that do not have implementations.</p>"},{"location":"porting-extensions/#some-low-level-apis-dont-enforce-locking","title":"Some low-level APIs don't enforce locking","text":"<p>Some low-level functions like <code>PyList_SET_ITEM</code> and <code>PyTuple_SET_ITEM</code> do not do any internal locking and should only be used to build newly created values. Do not use them to modify existing containers in the free-threaded build.</p>"},{"location":"porting-extensions/#limited-api-support","title":"Limited API support","text":"<p>The free-threaded build does not support the limited CPython C API. If you currently use the limited API to build wheels that do not depend on a specific Python version, you will not be able to use it while shipping binaries for the free-threaded build. In practice, the limited API is a subset of the full C API, so your extension will build, you just cannot set <code>Py_LIMITED_API</code> at build time. This also means that code inside <code>#ifdef Py_GIL_DISABLED</code> checks can use C API constructs outside the limited API if you would like to do that, although these uses will need to be removed once the free-threaded build gains support for compiling with the limited API.</p>"},{"location":"porting-extensions/#dependencies-that-dont-support-free-threading","title":"Dependencies that don't support free-threading","text":"<p>If one of your build or runtime dependencies do not support free-threading, (e.g. CFFI currently doesn't), you might be able to switch to a fork. Find more details in our guidance for handling dependencies that don't support free-threading.</p>"},{"location":"porting/","title":"Porting Python Packages to Support Free-Threading","text":"<p>This document discusses porting an existing Python package to support free-threading Python.</p>"},{"location":"porting/#current-status-as-of-early-2025","title":"Current status (as of early 2025)","text":"<p>Many Python packages, particularly packages relying on C extension modules, do not consider multithreaded use or make strong assumptions about the GIL providing sequential consistency in multithreaded contexts. These packages will:</p> <ul> <li>fail to produce deterministic results on the free-threaded build</li> <li>may, if there are C extensions involved, crash the interpreter in multithreaded use in ways that are impossible on the     GIL-enabled build</li> </ul> <p>Attempting to parallelize many workflows using the Python threading module will not produce any speedups on the GIL-enabled build, so thread safety issues that are possible even with the GIL are not hit often since users do not make use of threading as much as other parallelization strategies. This means many codebases have threading bugs that up-until-now have only been theoretical or present in niche use cases. With free-threading, many more users will want to use Python threads.</p> <p>This means we must analyze Python codebases to identify supported and unsupported multithreaded workflows and make changes to fix thread safety issues. Extra care must be taken to address this need, particularly when using low-level C, C++, Cython, and Rust code exposed to Python. Even pure Python codebases can exhibit non-determinism and races in the free-threaded build that are either very unlikely or impossible in the default configuration of the GIL-enabled build.</p> <p>For a more in-depth look at the differences between the GIL-enabled and free-threaded build, we suggest reading the <code>ft_utils</code> documentation on this topic.</p>"},{"location":"porting/#suggested-plan-of-attack","title":"Suggested Plan of Attack","text":"<p>Below, we outline a plan of attack for updating a Python project to support the free-threaded build. Since the changes required in native extensions are more substantial, we have split off the guide for porting extension modules into a subsequent section.</p>"},{"location":"porting/#define-and-document-thread-safety-guarantees","title":"Define and document thread safety guarantees","text":"<p>Consider adding a section to your documentation clearly documenting the thread safety guarantees of your library. Note any use of global state as well as whether the mutable data structures exposed by your library support sequentially consistent shared concurrent use. You should document any locks that you expect might impact multithreaded scaling for realistic workflows. Encourage user feedback, particularly for reports of thread-unsafe behavior in code that is documented to be thread-safe, as well as reports of poor multithreaded scaling in code that you expect to scale well.</p> <p>You can indicate the level of support for free-threading in your library by adding a trove classifier to the metadata of your package. The currently supported trove classifiers for this purpose are:</p> <ul> <li><code>Programming Language :: Python :: Free Threading :: 1 - Unstable</code></li> <li><code>Programming Language :: Python :: Free Threading :: 2 - Beta</code></li> <li><code>Programming Language :: Python :: Free Threading :: 3 - Stable</code></li> <li><code>Programming Language :: Python :: Free Threading :: 4 - Resilient</code></li> </ul> <p>The numeric level of support in the classifier corresponds to the level of support. To give some guidance as to what that means:</p> <ol> <li>For experimentation and feedback only.</li> <li>Free threaded usage is supported, but documentation of constraints and limitations may be incomplete.</li> <li>Supported for production use, multithreaded use is tested, and thread safety issues are clearly documented.</li> <li>Fully supported and fully thread safe.</li> </ol> <p>You can see how supporting the free-threaded build is not all all-or-nothing thing. It is a perfectly valid choice to, for example, only support running on the free-threaded build in effectively single-threaded contexts and not support shared use of objects. It is then up to the users of your library to add locking where appropriate or needed. The advantage of this choice is that it does not force all consumers of your library to pay any cost associated with ensuring thread safety.</p>"},{"location":"porting/#thread-safety-of-pure-python-code","title":"Thread Safety of Pure Python Code","text":"<p>The CPython interpreter protects you from low-level memory unsafety due to data races. It does not protect you from introducing thread safety issues due to race conditions. It is possible to write algorithms that depend on the precise timing of threads completing work. It is up to you as a user of multithreaded parallelism to ensure that simultaneous reads and writes to the same Python variable are impossible.</p> <p>Below we describe various approaches for improving the determinism of multithreaded pure Python code. The correct approach will depend on exactly what you are doing.</p>"},{"location":"porting/#general-considerations-for-porting","title":"General considerations for porting","text":"<p>Many projects assume the GIL serializes access to state shared between threads, introducing the possibility of data races in native extensions and race conditions that are impossible when the GIL is enabled.</p> <p>We suggest focusing on safety and multithreaded scaling before single-threaded performance.</p> <p>Here's an example of this approach. If adding a lock to a global cache would harm multithreaded scaling, and turning off the cache implies a small performance hit, consider doing the simpler thing and disabling the cache in the free-threaded build.</p> <p>Single-threaded performance can always be improved later, once you've established free-threaded support and hopefully improved test coverage for multithreaded workflows.</p> <p>NumPy, for example, decided not to add explicit locking to the ndarray object and does not support mutating shared ndarrays. This was a pragmatic choice given existing heavy multithreaded use of NumPy in the GIL-enabled build and a desire to not introducing scaling bottlenecks in existing workflows.</p> <p>Eventually NumPy may need to offer explicitly thread-safe data structures, but it is a valid choice to initially support free-threading while still exposing possibly unsafe operations if users use the library unsafely.</p> <p>For your libraries, we suggest to focus on thread safety issues that only occur with the GIL disabled. Any non-critical pre-existing thread safety issues can be dealt with later once the free-threaded build is used more. The goal for your initial porting effort should be to enable further refinement and experimentation by fixing issues that prevent using the library at all.</p>"},{"location":"porting/#multithreaded-python-programming","title":"Multithreaded Python Programming","text":"<p>The Python standard library offers a rich API for multithreaded programming. This includes the <code>threading</code> module, which offers relatively low-level locking and synchronization primitives, as well as the <code>queue module</code> for safe communication between threads, and the <code>ThreadPoolExecutor</code> high-level thread pool runner.</p> <p>If you'd like to learn more about multithreaded Python programming in the GIL-enabled build, Santiago Basulto's tutorial from PyCon 2020 is a good place to start.</p> <p>For a pedagogical introduction to multithreaded programming in free-threaded Python, we suggest reading the <code>ft_utils</code> documentation, particularly the section on the impact of the global interpreter lock on multithreaded Python programs. Many pure Python operations are not atomic and are susceptible to race conditions, or only appear to be thread-safe in the GIL-enabled build because of details of how CPython releases the GIL in a round-robin fashion to allow threads to run.</p>"},{"location":"porting/#dealing-with-mutable-global-state","title":"Dealing with mutable global state","text":"<p>The most common source of thread safety issues in Python packages is use of global mutable state. Many projects use module-level or class-level caches to speed up execution but do not envision filling the cache simultaneously from multiple threads. See the testing guide for strategies to add tests to detect problematic global state.</p> <p>For example, the <code>do_calculation</code> function in the following module is not thread-safe:</p> <pre><code>from internals import _do_expensive_calculation\n\nglobal_cache = {}\n\n\ndef do_calculation(arg):\n    if arg not in global_cache:\n        global_cache[arg] = _do_expensive_calculation(arg)\n    return global_cache[arg]\n</code></pre> <p>If <code>do_calculation</code> is called simultaneously in multiple threads, then it is possible for at least two threads to see that <code>global_cache</code> doesn't have the cached key and call <code>_do_expensive_calculation</code>. In some cases this is harmless, but depending on the nature of the cache, this could lead to unnecessary network access, resource leaks, or wasted unnecessary compute cost.</p>"},{"location":"porting/#converting-global-state-to-thread-local-state","title":"Converting global state to thread-local state","text":"<p>One way of dealing with issues like this is to convert a shared global cache into a thread-local cache. In this approach, each thread will see its own private copy of the cache, making races between threads impossible. This approach makes sense if having extra copies of the cache in each thread is not prohibitively expensive or does not lead to excessive runtime network, CPU, or memory use.</p> <p>In pure Python, you can create a thread-local cache using an instance of threading.local. Each thread will see independent versions of the thread-local object. You could rewrite the above example to use a thread-local cache like so:</p> <pre><code>import threading\n\nfrom internals import _do_expensive_calculation\n\nlocal = threading.local()\n\nlocal.cache = {}\n\n\ndef do_calculation(arg):\n    if arg not in local.cache:\n        local.cache[arg] = _do_expensive_calculation(arg)\n    return local.cache[arg]\n</code></pre> <p>This wouldn't help a case where each thread having a copy of the cache would be prohibitive, but it does fix possible issues with resource leaks issues due to races filling a cache.</p>"},{"location":"porting/#making-mutable-global-caches-thread-safe-with-locking","title":"Making mutable global caches thread-safe with locking","text":"<p>If a thread-local cache doesn't make sense, then you can serialize access to the cache with a lock. A lock provides exclusive access to some resource by forcing threads to acquire a lock instance before they can use the resource and release the lock when they are done. The lock ensures that only one thread at a time can use the acquired lock - all other threads block execution until the thread that holds the lock releases it, at which point only one thread waiting to acquire the lock is allowed to run.</p> <p>You could rewrite the above thread-unsafe example to be thread-safe using a lock like this:</p> <pre><code>import threading\n\nfrom internals import _do_expensive_calculation\n\ncache_lock = threading.Lock()\nglobal_cache = {}\n\n\ndef do_calculation(arg):\n    if arg in global_cache:\n        return global_cache[arg]\n\n    with cache_lock:\n        if arg not in global_cache:\n            global_cache[arg] = _do_expensive_calculation(arg)\n    return global_cache[arg]\n</code></pre> <p>Note that after acquiring the lock, we first check if the requested key has been filled by another thread, to prevent unnecessary calls to <code>_do_expensive_calculation</code> if another thread filled the cache while the thread currently holding the lock was blocked on acquiring the lock. Also note that we avoid using <code>Lock.acquire</code> and <code>Lock.release</code> and instead we use the lock as a context manager. The difference is subtle: the context manager calls <code>Lock.release</code> in a <code>try</code> ... <code>finally</code> clause, so if <code>_do_expensive_calculation</code> were to raise an exception, this ensures that the lock won't stay locked forever.</p> <p>Note that acquiring the same lock recursively leads to deadlocks. Also, in general, it is possible to create a deadlock in any program with more than one lock. Care must be taken to ensure that operations done while the lock is held cannot lead to recursive calls or lead to a situation where a thread owning the lock is blocked on acquiring a different mutex. You do not need to worry about deadlocking with the GIL in pure Python code, the interpreter will handle that for you. There is also threading.RLock, which provides a reentrant lock allowing threads to recursively acquire the same lock.</p> <p>Finally, note how the above code will ensure that only a single call to <code>_do_expensive_calculation</code> will run at any given time, regardless of <code>arg</code>. This may not be desirable; one could want to allow calling the function in parallel for different arguments. This however would require a substantially more complex locking pattern.</p>"},{"location":"porting/#raising-errors-under-shared-concurrent-use","title":"Raising errors under shared concurrent use.","text":"<p>Sometimes it's a programming error to share an object between threads. An example might be a wrapper for a low-level C compression library that does not support sharing compression contexts between threads. You could make it so users see an error at runtime when they try to share a compression context like this:</p> <pre><code>from dataclasses import dataclass\n\n\n@dataclass\nclass CompressionContext:\n    lock: threading.Lock\n    state: _LowLevelCompressionContext\n\n    def compress(self, data):\n        if not self.lock.acquire(blocking=False):\n            raise RuntimeError(\"Concurrent use detected!\")\n        try:\n            self.state.compress(data)\n        finally:\n            self.lock.release()\n</code></pre> <p>This does require paying the cost of acquiring and releasing a mutex, but because no thread ever blocks on acquiring the lock, this thread cannot introduce hidden multithreaded scaling issues.</p>"},{"location":"porting/#dealing-with-thread-unsafe-objects","title":"Dealing with thread-unsafe objects","text":"<p>Mutability of objects is deeply embedded in the Python runtime and many tools freely assign to or mutate data stored in a python object.</p> <p>In the GIL-enabled build, in many cases, you can get away with mutating a shared object safely. This is true so long as whatever mutation you are attempting to do is fast enough that a thread switch is very unlikely to happen while you are doing work.</p> <p>In the free-threaded build there is no GIL to protect against mutation of state living on a Python object that is shared between threads. Just like when we used a lock to protect a global cache, we can also use a per-object lock to serialize access to state stored in a Python object. Consider the following class:</p> <pre><code>import time\nimport random\n\n\nclass RaceyCounter:\n    def __init__(self):\n        self.value = 0\n\n    def increment(self):\n        current_value = self.value\n        time.sleep(random.randint(0, 10) * 0.0001)\n        self.value = current_value + 1\n</code></pre> <p>Here we're simulating doing an in-place addition on an expensive function. A real example might have a method that looks something like this:</p> <pre><code>def increment(self):\n    self.value = do_some_expensive_calulation(self.value)\n</code></pre> <p>If we run this example in a thread pool, you'll see that the answer you get will vary randomly depending on the timing of the sleeps:</p> <pre><code>from concurrent.futures import ThreadPoolExecutor\n\ncounter = RaceyCounter()\n\n\ndef closure(counter):\n    counter.increment()\n\n\nwith ThreadPoolExecutor(max_workers=8) as tpe:\n    futures = [tpe.submit(closure, counter) for _ in range(1000)]\n    for f in futures:\n        f.result()\n\nprint(counter.value)\n</code></pre> <p>On both the free-threaded and GIL-enabled build, you will see the output of this script randomly vary.</p> <p>We can ensure the above script has deterministic answers by adding a lock to our counter:</p> <pre><code>import threading\n\n\nclass SafeCounter:\n    def __init__(self):\n        self.value = 0\n        self.lock = threading.Lock()\n\n    def increment(self):\n        with self.lock:\n            current_value = self.value\n            time.sleep(random.randint(0, 10) * 0.0001)\n            self.value = current_value + 1\n</code></pre> <p>If you replace <code>RaceyCounter</code> with <code>SafeCounter</code> in the script above, it will always output 1000.</p> <p>Of course this introduces a scaling bottleneck when <code>SafeCounter</code> instances are concurrently updated. It's possible to implement more optimized locking strategies, but doing so requires knowledge of the problem.</p>"},{"location":"porting/#third-party-libraries","title":"Third-party libraries","text":"<p>Both the <code>ft_utils</code> and <code>cereggii</code> libraries offer data structures that add enhanced atomicity or improved multithreaded scaling compared with standard library primitives.</p>"},{"location":"porting/#dependencies-that-dont-support-free-threading","title":"Dependencies that don't support free-threading","text":"<p>If one of your package's dependencies do not support free-threading, you might be able to switch to a fork that does. Find more details in our guidance for handling dependencies that don't support free-threading.</p>"},{"location":"profiling/","title":"Multithreaded Profiling with samply","text":"<p>Currently, we recommend using samply, a low-level sampling profiler, to identify performance bottlenecks. You can download binaries from GitHub or build and install locally using a Rust toolchain. See the samply GitHub page for more details.</p> <p>While you can also use <code>perf</code> or <code>dtrace</code> to do similar work, samply provides a uniform multi-platform interface for this work. You use samply from the command-line. For example, to record a trace of the execution of a Python script, you could do:</p> <pre><code>samply record python test.py\n</code></pre> <p>Note that the <code>python</code> here must resolve to a real Python executable.</p> <p>You can also attach to a running process if you have a PID:</p> <pre><code>samply record --pid $PROCESS_PID\n</code></pre> <p>Both of these commands instrument the Python interpreter to record the call stack at regular intervals. Samply then collates this information in a standard format and opens it in a local version of the Firefox profiler.</p> <p>Once the script is finished - either by exiting or by being interrupted with Ctrl-C - samply will print out a report and open a web browser pointing at the profile browser.</p> <p>Briefly, the interface is broken up into two pieces: a timeline in which the CPU utilization appears as a graph and a set of panels that allow diving into the callstack at individual instants in the timeline or aggregated over selections in the timeline. It's particularly useful to look at the flame graph and stack chart panels. The former allows identifying calls that are particularly expensive while the latter allows visualization of how the call stack changes with time.</p> <p>See the Firefox profiler documentation for more details about how to use the interface.</p> <p>Note that by default, you will only see information about native frames. Generally, the call stack will include frames from inside the CPython interpreter, which will eventually call into a native extension outside the interpreter, if there is one. If you are analyzing the performance of code that is pure-Python or goes back and forth between C and Python, you may want to set up a Linux development environment and follow the instructions below to set up profiling both native and Python frames in the same profile output.</p>"},{"location":"profiling/#python-stack-frames-on-linux","title":"Python stack frames on Linux","text":"<p>If you have access to a Linux machine and a Python development environment, then you can turn on Python's support for generating perf events by running Python with the <code>-X perf</code> command-line option:</p> <pre><code>samply record python -X perf test.py\n</code></pre> <p>The resulting profile will include annotations with information about the Python frames. In the profile browser, Python frames show up in light blue and native frames show up in yellow.</p>"},{"location":"profiling/#uploading-profiler-data-to-profilerfirefoxcom","title":"Uploading profiler data to profiler.firefox.com","text":"<p>The neat thing about using the Firefox profiler as the UI for samply is it means sharing profiling information over the internet is as easy as clicking a button in the UI.</p> <p>By default, samply runs the firefox profiler locally and does not share data it collects. You can optionally click the upload button in the top-right corner of the profiler interface to upload the profile data and generate a permalink. This uploads the data to profiler.firefox.com and others can view the exact same profiler interface as the one you see locally. This can be a powerful tool to get help if you don't understand what you are looking at.</p>"},{"location":"resources/","title":"More Resources","text":"<p>Apart from this website, there's a wide list of resources on the free-threaded build and free-threading topics in general, including documentation, blog posts, conference talks, and others. We'll try to keep an up-to-date list here:</p>"},{"location":"resources/#cpython-documentation","title":"CPython documentation","text":"<ul> <li>Python experimental support for free threading</li> <li>C API Extension Support for Free Threading HOWTO on docs.python.org</li> </ul>"},{"location":"resources/#free-threading-pre-history-and-background","title":"Free-threading pre-history and background","text":"<ul> <li>PEP 703 design document</li> <li>PEP 703 initial discussion thread, as     well as the follow-up discussion thread</li> <li>PEP 703 acceptance announcement</li> </ul>"},{"location":"resources/#community-maintained-packages","title":"Community-maintained packages","text":"<ul> <li><code>ft_utils</code> documentation</li> <li><code>cereggii</code></li> </ul>"},{"location":"resources/#conference-talks","title":"Conference talks","text":""},{"location":"resources/#pycon-2025","title":"PyCon 2025","text":"<ul> <li>\"Unraveling Community Support for Free-Threaded Python\" by Lysandros Nikoloau and Nathan Goldbaum</li> <li>\"High-Performance Python: Faster Type Checking and Free Threaded Execution\" by Sam Gross and Neil Mitchell</li> <li>\"Using Rust in Free-Threaded vs Regular Python 3.13\" by David Hewitt</li> <li>\"Building a NoGIL Load Balancer in 30 minutes\" by Alvaro Duran</li> </ul>"},{"location":"resources/#europython-2022","title":"EuroPython 2022","text":"<ul> <li>\"Keynote: Multithreaded Python without the GIL\"</li> </ul>"},{"location":"resources/#community-blog-posts","title":"Community blog posts","text":"<ul> <li>Simon Willison's post about trying out free-threaded Python on macOS</li> <li>Codspeed's blog post about free-threading performance</li> <li>NVIDIA's blog post about threaded data loading</li> <li>Quansight Labs blog post about start of work on free-threading</li> <li>Quansight Labs blog post about one year of work on free-threading</li> </ul>"},{"location":"resources/#cpython-internals","title":"CPython internals","text":"<p>There's also a lot of useful resources on CPython internals, that are not specific to the free-threaded build:</p> <ul> <li>CPython internal docs</li> <li>Dated tutorial on writing C extension modules</li> <li>Python behind the scenes series</li> <li>\u0141ukasz Langa's PyCon Thailand talk on the Python 3.13 release</li> <li>Anthony Shaw's PyCon US talk on free-threading and other parallelism concepts</li> </ul>"},{"location":"running-gil-disabled/","title":"Running Python with the GIL Disabled","text":"<p>Info</p> <p>Most of the content on this page is also covered in the Python 3.13 release notes.</p> <p>Note</p> <p>The free-threaded Python executable will always have a <code>python3.13t</code> alias (for Python 3.13); whether <code>python</code>, <code>python3</code> and/or <code>python3.13</code> point at the free-threaded executable depends on the installation method used (see the install guide for more details).</p> <p>For example, the Python 3.13 Windows installer from python.org installs the free-threaded binary as <code>python3.13t.exe</code> (with a \"t\" suffix to indicate it is \"t\"hreaded), whereas the standard GIL-enabled Python binary is simply named <code>python.exe</code> (as usual). If you cannot find the free-threaded binary, that means the free-threaded option was not selected during installation.</p>"},{"location":"running-gil-disabled/#check-if-the-gil-is-disabled","title":"Check if the GIL is disabled","text":""},{"location":"running-gil-disabled/#from-the-command-line","title":"From the command line","text":"<p>You can verify if your build of CPython itself has the GIL disabled with the following incantation:</p> <pre><code>python -VV\n</code></pre> <p>If you are using Python 3.13b1 or newer, you should see a message like:</p> <pre><code>Python 3.13.1 experimental free-threading build (main, Dec 10 2024, 14:07:41) [Clang 16.0.0 (clang-1600.0.26.4)]\n</code></pre>"},{"location":"running-gil-disabled/#at-runtime","title":"At runtime","text":"<p>To verify whether the GIL is disabled at runtime or not, you can use this in your code:</p> <pre><code>import sys\n\nsys._is_gil_enabled()\n</code></pre> <p>This command will return <code>True</code> on the free-threaded build when the GIL is re-enabled at runtime, and should return <code>False</code> before importing any packages. Note that <code>sys._is_gil_enabled()</code> is only available on Python 3.13 and newer, you will see an <code>AttributeError</code> on older Python versions.</p>"},{"location":"running-gil-disabled/#force-the-gil-to-be-disabled","title":"Force the GIL to be disabled","text":"<p>To force Python to keep the GIL disabled even after importing a module that does not support running without it, use the <code>PYTHON_GIL</code> environment variable or the <code>-X gil</code> command line option:</p> <pre><code># these commands are equivalent\nPYTHON_GIL=0 python\n\npython -Xgil=0\n</code></pre>"},{"location":"running-gil-disabled/#check-if-using-a-free-threaded-build","title":"Check if using a free-threaded build","text":"<p>To check whether the Python interpreter you're using is a free-threaded build, irrespective of whether the GIL was re-enabled at runtime or not, you can use this within your code:</p> <pre><code>import sysconfig\n\nis_freethreaded = bool(sysconfig.get_config_var(\"Py_GIL_DISABLED\"))\n</code></pre>"},{"location":"testing/","title":"Validating thread safety with testing","text":"<p>Put priority on thread safety issues surfaced by real-world testing. Run the test suite for your project and fix any failures that occur only with the GIL disabled. Some issues may be due to changes in Python 3.13 that are not specific to the free-threaded build.</p> <p>If you are unable to run your package with the GIL disabled because of problems in extension modules or in dependencies, you can still test with the GIL enabled by setting the thread switch interval to a very small value (e.g. a microsecond or shorter). You can call <code>sys.setswitchiterval</code> before running multithreaded tests to force Python to release the GIL more often that the default configuration. This can expose thread safety issues that the GIL is masking.</p> <p>Unless your tests make heavy use of the <code>threading</code> module, you will likely not hit many issues, so also consider constructing multithreaded tests to expose bugs based on workflows you want to support. Issues found in these tests are the issues your users will most likely hit first.</p> <p>Multithreaded Python programs can exhibit race conditions which produce random results depending on the order of execution in a multithreaded context. This can happen even with the GIL providing locking, so long as the algorithm releases the GIL at some point, and many Python operations can lead to the GIL being released at some point. If your library was not designed with multithreading in mind, it is likely that some form of locking or synchronization is necessary to make mutable data structures defined by your library thread-safe. You should document the thread-safety guarantees of your library, both with and without the GIL.</p> <p>You should focus your efforts on analyzing the safety of shared use of mutable data structures or mutable global state. Decide whether it is supported and to what level it is supported to share mutable state between threads. It is a valid choice to leave it up to users to add synchronization, with the proviso that thread-unsafe data structures should be clearly documented as such.</p> <p>Generally global mutable state is not safe in the free-threaded build without some form of locking. Many projects use global mutable state (e.g. module-level or class-level state) for convenience with the assumption that the GIL provides locking on the state. That will most likely not be valid without some form of explicit locking on the free-threaded build. It is also likely that there are latent thread-safety issues related to use of global state even in the GIL-enabled build.</p> <p>Many test suites are implemented using global mutable state or assume that tests cannot run simultaneously. See the section on global state in tests for more information about updating test suites to work with the free-threaded build and dealing with tests that become flaky when run in a thread pool.</p> <p>You can look at pytest-run-parallel as well as pytest-freethreaded, which both offer pytest plugins to enable running tests in an existing <code>pytest</code> test suite simultaneously in many threads, with the goal of validating thread safety. unittest-ft offers similar functionality for running <code>unittest</code>-based tests in parallel.</p> <p>These plugins are useful for discovering issues related to use of global state, but cannot discover issues from multithreaded use of data structures defined by your library.</p> <p>If you would like to create your own testing utilities, the <code>concurrent.futures.ThreadPoolExecutor</code> class is a lightweight way to create multithreaded tests where many threads repeatedly call a function simultaneously. You can also use the <code>threading</code> module directly for more complicated multithreaded test workflows. Adding a <code>threading.Barrier</code> before a line of code that you suspect will trigger a race condition is a good way to synchronize workers and increase the chances that an infrequent test failure will trigger.</p> <p>NumPy makes use of the following helper function to enable writing explicitly multithreaded tests, with a number of useful features to generically set up different testing scenarios:</p> <pre><code>from concurrent.futures import ThreadPoolExecutor\nimport threading\n\n\ndef run_threaded(\n    func,\n    num_threads=8,\n    pass_count=False,\n    pass_barrier=False,\n    outer_iterations=1,\n    prepare_args=None,\n):\n    \"\"\"Runs a function many times in parallel\"\"\"\n    for _ in range(outer_iterations):\n        with ThreadPoolExecutor(max_workers=num_threads) as tpe:\n            if prepare_args is None:\n                args = []\n            else:\n                args = prepare_args()\n            if pass_barrier:\n                barrier = threading.Barrier(num_threads)\n                args.append(barrier)\n            if pass_count:\n                all_args = [(func, i, *args) for i in range(num_threads)]\n            else:\n                all_args = [(func, *args) for i in range(num_threads)]\n            try:\n                futures = []\n                for arg in all_args:\n                    futures.append(tpe.submit(*arg))\n            finally:\n                if len(futures) &lt; num_threads and pass_barrier:\n                    barrier.abort()\n            for f in futures:\n                f.result()\n</code></pre> <p>Using this helper, you could write a multithreaded test using a shared list like this:</p> <pre><code>def test_parallel_append():\n    shared_list = []\n\n    def closure(i, b):\n        b.wait()\n        shared_list.append(i)\n\n    run_threaded(closure, num_threads=8, pass_barrier=True, pass_count=True)\n\n    assert sum(shared_list) == sum(range(8))\n</code></pre> <p>Generally multithreaded tests look something like the above: define a closure that operates on (possibly) shared data, spawn a thread pool that runs the closure in many threads, and assert something about the state of the world either inside the closure or after the thread pool finishes running. The assertion might be merely that a crash doesn't happen, in which case no explicit asserts are necessary.</p> <p>Tests that fail due to thread safety issues are inherently flaky. You should not be surprised to see tests that pass or fail randomly, or even fail a very small percentage of the time. When writing multithreaded tests your goal should be to maximize the chances of triggering a thread safety issue. You could pass <code>outer_iterations</code> to <code>run_threaded</code> to multiply the number of chances a thread triggers a thread safety issue in a single test.</p>"},{"location":"testing/#fixing-thread-unsafe-tests","title":"Fixing thread-unsafe tests","text":"<p>Many existing tests are written using global state. This is not a problem if the test only runs once, but if you would like to use your tests to check for possible thread safety issues by running existing tests on many threads, you will likely need to update the tests to eliminate use of global state.</p> <p>Since tests using global state are inherently racey, this means that test failures associated with these tests are also inherently flakey. If you see tests failing intermittently, you should not discount that you are using global state in a test, or even inadvertently using global state in <code>pytest</code> itself.</p>"},{"location":"testing/#pytest-is-not-thread-safe","title":"<code>pytest</code> is not thread-safe","text":"<p>See the <code>pytest</code> docs for more information about this. While tests can manage their own threads, you should not assume that functionality provided by <code>pytest</code> is thread-safe.</p> <p>Functionality that is known not to be thread-safe includes:</p> <ul> <li><code>pytest.warns</code> on Python 3.13 and older,     it relies on <code>warnings.catch_warnings</code>, which is not thread-safe until Python 3.14, and even then only on the free-threaded build by default.</li> <li>The <code>tmp_path</code>     and <code>tmpdir</code>     fixtures, since they rely on the filesystem</li> <li>The <code>capsys</code>     fixture,     because of shared use of <code>sys.stdout</code> and <code>sys.stderr</code>.</li> <li>The <code>monkeypatch</code>     fixture,     since monkeypatching inherently modifies global state in a class or module.</li> </ul> <p>Note that the <code>pytest</code> maintainers have explicitly ruled out making <code>pytest</code> thread-safe, please do not open issues asking to fix thread safety issues in <code>pytest</code> itself.</p>"},{"location":"testing/#the-warnings-module-is-not-thread-safe-before-python-314","title":"The <code>warnings</code> module is not thread-safe before Python 3.14","text":"<p>Many tests carefully ensure that warnings will be seen by the user in cases where the library author intends users to see them. These tests inevintably make use of the <code>warnings</code> module. As noted in the documentation for <code>warnings.catch_warnings</code> on older Python versions, the functionality provided by Python to track warnings was inherently thread-unsafe.</p> <p>If you are running tests under <code>pytest-run-parallel</code>, it will automatically mark tests that use warnings as thread-unsafe when running on Python 3.13 and older or on 3.14 when the interpreter is not configured to use thread-safe warnings. Free-threaded Python 3.14 enabled thread-safe warnings by default, but it is not yet the default on the GIL-enabled interpreter.</p> <p>If you are using another mechanism to execute multithreaded tests, you will need to skip any checks for warnings if the interpreter is not configured for thread-safe warnings.</p> <p>See the documentation for the new <code>context_aware_warnings</code> and <code>thread_inherit_context</code> Python configuration options as well as the discussion in the \"what's new\" entry for Python 3.14.</p>"},{"location":"testing/#file-system-thread-safety","title":"File system thread safety","text":"<p>Many tests make use of the file system, either via a temporary file, or by simply directly writing to the folder running the test. If the filename used by the test is a constant or it is ever shared between instances of the test, the filesystem becomes shared global state, and the test will not be thread-safe.</p> <p>The easiest way to fix this is to use <code>tempfile</code>, which automatically handles generating file handles in a thread-safe manner. If for some reason this isn't practical, consider forcing the filenames used in tests to be unique, for example by appending a UUID to the filename.</p>"},{"location":"tracking/","title":"Compatibility Status Tracking","text":"<p>This page tracks the status of packages for which we're aware of active work on free-threaded support. It contains packages with extension modules, as well as build tools and packages that needed code specifically to support free-threading. Note that pure Python code works without changes by design, hence this page does not aim to track pure Python packages.</p> <p>We are updating this tracking table manually and including links to nightlies and project-specific issue links. There is also an automatically updated tracker that pulls in information for a wider range of packages, but only tracks whether or not they have wheels on PyPI.</p> <p>If there's a bug related to free-threading in a library you use, please open an issue on the corresponding issue tracker or post a comment on the corresponding free-threading support tracking issue (see table below). If an issue spans multiple projects or there's an ecosystem-wide point to discuss, please open an issue on this issue tracker.</p> Project Upstream issue Tested in CI PyPI release First version with support Nightly wheels Nightly link aiohttp Bazel (rules-python) <sup>1</sup> 0.39.0 argon2-cffi-bindings Boost.Python bcrypt 4.3.0 bottleneck 1.5.0 brotli cffi charset-normalizer cibuildwheel 2.19 CMake 3.30.0 conda 24.11.0 ContourPy 1.3.0 cramjam 2.11.0 cryptography Cython 3.1.0 frozenlist 1.6.0 grpcio hatch h5py hf-xet httptools hypothesis JAX 0.5.1 joblib 1.4.2 jupyterlab kiwisolver 1.4.8 kornia-rs 0.1.9 LibCST 1.8.0 lxml lz4 mamba markupsafe 3.0.0 matplotlib 3.9.0 maturin 1.7.5 Meson 1.5.0 meson-python 0.16.0 ml-dtypes 0.5.1 mlir-python 20.1.0 multidict 6.2.0 msgpack msgspec mypyc nanobind 2.2.0 ndindex 1.10.0 nox Nuitka numexpr 2.11.0 NumPy 2.1.0 nvImageCodec 0.4.0 ONNX 1.18.0 OpenCV orjson packaging 24.0 pandas 2.2.3 Pillow 11.0.0 pip 24.1 Pixi 0.39.5 polars propcache 0.3.0 protobuf psutil psycopg pydantic 2.11.0 pydantic-core 2.29.0 PyArrow 18.0.0 PyAV pybind11 2.13 pycares pygit2 PyNaCl PyO3 <sup>2</sup> 0.23 PyObjC 11.0 Pythran 0.18.0 PyTorch 2.6.0 PyWavelets 1.7.0 pywinpty 2.0.15 PyYAML PyYAML-ft <sup>3</sup> 7.0.0 PyZMQ :simple-github 27.0.0 RapidFuzz rpds-py 0.22.3 ruamel.yaml.clib rust-numpy <sup>3</sup> 0.24.0 safetensors scikit-build-core 0.9.5 scikit-image 0.25.2 scikit-learn 1.6.0 SciPy 1.15.0 sentencepiece setproctitle 1.3.6 setuptools 69.5.0 setuptools-rust 1.11.0 Shapely 2.1.0 spacy sqlalchemy srsly SWIG 4.4.0 thrift tokenizers tornado tox 4.26.0 uv 0.4.24 wrapt 1.17.0 xxhash yarl 1.20.0 zstandard <ol> <li> <p>Release available in the Bazel Central Registry \u21a9</p> </li> <li> <p>Rust library released on crates.io \u21a9</p> </li> <li> <p>PyYAML-ft is a fork of PyYAML. For more details on how to use it, see our PyYAML-specific section on dependencies that don't support free-threading.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"examples/","title":"Examples Demonstrating Free-Threaded Python","text":"<p>This page gathers examples showing how to use free-threaded Python to speed up code using the Python <code>threading</code> module. In all cases, the free-threaded build outperforms the GIL-enabled build since the GIL-enabled build does not scale well due to lock contention.</p> <ul> <li>Mandelbrot Set Visualization</li> <li>Monte Carlo Simulation</li> <li>Web Scraping with asyncio</li> </ul> <p>External examples:</p> <ul> <li>Improved Data Loading with Threads</li> <li>Using <code>AtomicDict.reduce()</code> for Multithreaded Aggregation</li> </ul> <p>We'd love to have more examples! See the contribution guide if you're interested in adding more use-cases that show off the free-threaded build.</p>"},{"location":"examples/asyncio/","title":"Web Scraping with asyncio","text":"<p>Web scraping is the process of extracting useful data from websites, and it becomes especially challenging and time-consuming when dealing with hundreds or thousands of pages. The traditional synchronous approach scrapes only one page at a time and is slow. With asyncio, we can leverage asynchronous I/O to scrape multiple pages concurrently, which significantly speeds up the process; however, asyncio can only utilize a single CPU core.</p> <p>Modern computers have multiple CPU cores; yet, asyncio only takes advantage of a single core. However, with free-threaded Python, we can run multiple asyncio workers in threads to utilize all available cores.</p>"},{"location":"examples/asyncio/#web-scraping-example-with-free-threaded-python","title":"Web scraping example with free-threaded Python","text":"<p>This example demonstrates how to use free-threaded Python to run multiple asyncio workers in parallel, allowing us to scrape numerous pages concurrently across multiple cores.</p> <p>It uses <code>aiohttp</code> for asynchronous HTTP requests and <code>bs4</code> for parsing HTML. The example script scrapes Hacker News stories and their comments, demonstrating how to efficiently scrape a large number of pages using asyncio and free-threaded Python.</p> <ol> <li> <p>Install the required packages with:</p> <pre><code>pip install aiohttp beautifulsoup4\n</code></pre> </li> <li> <p>Create the script file:</p> <pre><code># scraper.py\n\nimport aiohttp\nimport asyncio\nfrom bs4 import BeautifulSoup\nfrom queue import Queue, Empty\nfrom concurrent.futures import ThreadPoolExecutor\nfrom time import perf_counter\nfrom argparse import ArgumentParser\n\nBASE_URL = \"https://news.ycombinator.com/news?p={}\"\nITEM_URL = \"https://news.ycombinator.com/item?id={}\"\n\n\nasync def fetch(session: aiohttp.ClientSession, url: str) -&gt; str:\n    async with session.get(url, timeout=100) as response:\n        return await response.text()\n\n\ndef parse_stories(html: str) -&gt; list[dict]:\n    soup = BeautifulSoup(html, \"html.parser\")\n    stories = []\n\n    for item in soup.select(\".athing\"):\n        title_tag = item.select_one(\".titleline &gt; a\")\n        story_id = item.get(\"id\")\n\n        if title_tag and story_id:\n            title = title_tag.text.strip()\n            link = title_tag[\"href\"].strip()\n            stories.append({\"id\": story_id, \"title\": title, \"link\": link})\n\n    return stories\n\n\ndef parse_comments(html: str) -&gt; list[dict]:\n    soup = BeautifulSoup(html, \"html.parser\")\n    comments = []\n\n    for row in soup.select(\"tr.comtr\"):\n        user_tag = row.select_one(\".hnuser\")\n        comment_tag = row.select_one(\".commtext\")\n\n        if user_tag and comment_tag:\n            user = user_tag.text.strip()\n            text = comment_tag.get_text(separator=\" \", strip=True)\n            comments.append({\"user\": user, \"text\": text})\n\n    return comments\n\n\nasync def fetch_story_with_comments(\n    session: aiohttp.ClientSession, story: dict\n) -&gt; dict:\n    comment_html = await fetch(session, ITEM_URL.format(story[\"id\"]))\n    story[\"comments\"] = parse_comments(comment_html)\n    return story\n\n\nasync def worker(queue: Queue, all_stories: list) -&gt; None:\n    async with aiohttp.ClientSession() as session:\n        while True:\n            async with asyncio.TaskGroup() as tg:\n                try:\n                    page = queue.get(block=False)\n                except Empty:\n                    break\n                html = await fetch(session, page)\n                stories = parse_stories(html)\n                if not stories:\n                    break\n                for story in stories:\n                    tg.create_task(fetch_story_with_comments(session, story))\n            all_stories.extend(stories)\n\n\ndef main(multithreaded: bool) -&gt; None:\n    queue = Queue()\n    all_stories = []\n    for page in range(1, 101):\n        queue.put(BASE_URL.format(page))\n    start_time = perf_counter()\n    if multithreaded:\n        print(\"Using multithreading for fetching stories...\")\n        workers: int = 8  # no of CPU cores to use\n        with ThreadPoolExecutor(max_workers=workers) as executor:\n            for _ in range(workers):\n                executor.submit(lambda: asyncio.run(worker(queue, all_stories)))\n    else:\n        print(\"Using single thread for fetching stories...\")\n        asyncio.run(worker(queue, all_stories))\n    end_time = perf_counter()\n    print(\n        f\"Scraping speed: {len(all_stories) / (end_time - start_time):.0f} stories/sec\"\n    )\n\n\nif __name__ == \"__main__\":\n    parser = ArgumentParser(description=\"Scrape Hacker News stories and comments.\")\n    parser.add_argument(\n        \"--multithreaded\",\n        action=\"store_true\",\n        default=False,\n        help=\"Use multithreading for fetching stories.\",\n    )\n    args = parser.parse_args()\n    main(args.multithreaded)\n</code></pre> </li> <li> <p>Run the script with single thread:</p> <pre><code>python scraper.py\n</code></pre> </li> <li> <p>Run the script with multiple threads by using the <code>--multithreaded</code> flag:</p> <pre><code>python scraper.py --multithreaded\n</code></pre> </li> <li> <p>Run the script using free-threaded Python with multiple threads:</p> <pre><code>python -X gil=0 scraper.py --multithreaded\n</code></pre> </li> </ol>"},{"location":"examples/asyncio/#example-results-and-explanation","title":"Example results and explanation","text":"<p>Compare the performance results of each script run using a 12-core CPU:</p> Configuration Stories/sec default build, single thread 12 default build, multithreaded 35 free-threaded build, multithreaded 80 <p>The default build performs better with multiple threads than with a single thread, because Python releases the GIL during I/O operations. This allows other threads to run while a thread is waiting for network responses. This leads to some parallelism, but it is limited by the Global Interpreter Lock (GIL).</p> <p>The free-threaded build enables true parallelism across multiple cores, significantly increasing scraping speed. This example demonstrates how free-threaded Python can be used to efficiently scrape large amounts of data from the web by leveraging multiple CPU cores.</p>"},{"location":"examples/mandelbrot-threads/","title":"Mandelbrot threads","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\n</pre> import numpy as np import matplotlib.pyplot as plt import matplotlib In\u00a0[2]: Copied! <pre># Setting parameters (these values can be changed)\nnrows = 1000\nncols = 1000\nxcenter = -0.4601222\nycenter = .570286\nbound   = .002\nx_domain, y_domain = np.linspace(xcenter-bound, xcenter+bound, ncols), np.linspace(ycenter-bound, ycenter+bound, nrows)\nmax_iterations = 2000  # any positive integer value\ncolormap = \"nipy_spectral\"  # set to any matplotlib valid colormap\n\n\ndef mandelbrot(x, y):\n    z = 0\n    p = 2\n    c = complex(x, y)\n    for iteration_number in range(max_iterations):\n        if abs(z) &gt;= 2:\n            return iteration_number\n        else:\n            z = z**p + c\n    return 0\n\ndef plot_data(iteration_array):\n    # Plotting the data\n    ax = plt.axes()\n    ax.set_aspect(\"equal\")\n    graph = ax.pcolormesh(x_domain, y_domain, iteration_array, cmap=colormap,\n                          norm=matplotlib.colors.LogNorm())\n    plt.colorbar(graph)\n    plt.xlabel(\"Real-Axis\")\n    plt.ylabel(\"Imaginary-Axis\")\n    plt.show()\n</pre> # Setting parameters (these values can be changed) nrows = 1000 ncols = 1000 xcenter = -0.4601222 ycenter = .570286 bound   = .002 x_domain, y_domain = np.linspace(xcenter-bound, xcenter+bound, ncols), np.linspace(ycenter-bound, ycenter+bound, nrows) max_iterations = 2000  # any positive integer value colormap = \"nipy_spectral\"  # set to any matplotlib valid colormap   def mandelbrot(x, y):     z = 0     p = 2     c = complex(x, y)     for iteration_number in range(max_iterations):         if abs(z) &gt;= 2:             return iteration_number         else:             z = z**p + c     return 0  def plot_data(iteration_array):     # Plotting the data     ax = plt.axes()     ax.set_aspect(\"equal\")     graph = ax.pcolormesh(x_domain, y_domain, iteration_array, cmap=colormap,                           norm=matplotlib.colors.LogNorm())     plt.colorbar(graph)     plt.xlabel(\"Real-Axis\")     plt.ylabel(\"Imaginary-Axis\")     plt.show() In\u00a0[3]: Copied! <pre>%%time\niteration_array = np.zeros((nrows, ncols))\nfor (i, x) in enumerate(x_domain):\n    for (j, y) in enumerate(y_domain):\n        iteration_array[j, i] = mandelbrot(x, y)\n</pre> %%time iteration_array = np.zeros((nrows, ncols)) for (i, x) in enumerate(x_domain):     for (j, y) in enumerate(y_domain):         iteration_array[j, i] = mandelbrot(x, y) <pre>CPU times: user 15.9 s, sys: 6.87 ms, total: 16 s\nWall time: 16 s\n</pre> In\u00a0[4]: Copied! <pre>plot_data(iteration_array)\n</pre> plot_data(iteration_array) In\u00a0[5]: Copied! <pre>import sys\n\nprint(sys._is_gil_enabled())\n</pre> import sys  print(sys._is_gil_enabled()) <pre>False\n</pre> In\u00a0[6]: Copied! <pre>def worker(j_y):\n    for (i, x) in enumerate(x_domain):\n        for (j, y) in j_y:\n            iteration_array[j, i] = mandelbrot(x, y)\n</pre> def worker(j_y):     for (i, x) in enumerate(x_domain):         for (j, y) in j_y:             iteration_array[j, i] = mandelbrot(x, y) In\u00a0[7]: Copied! <pre>from concurrent.futures import ThreadPoolExecutor\nimport concurrent.futures\nimport itertools\n\ndef run_thread_pool(num_workers):\n    with ThreadPoolExecutor(max_workers=num_workers) as tpe:\n        chunks = itertools.batched(enumerate(y_domain), 4, strict=True)\n        try:\n            futures = [tpe.submit(worker, arg) for arg in chunks]\n            # block until all work finishes\n            concurrent.futures.wait(futures)\n        finally:\n            # check for exceptions in worker threads\n            [f.result() for f in futures]\n\nfor i in range(8):\n    iteration_array = np.zeros((nrows, ncols))\n    print(f\"{2**i} workers:\")\n    %time run_thread_pool(2**i)\n    plot_data(iteration_array)\n</pre> from concurrent.futures import ThreadPoolExecutor import concurrent.futures import itertools  def run_thread_pool(num_workers):     with ThreadPoolExecutor(max_workers=num_workers) as tpe:         chunks = itertools.batched(enumerate(y_domain), 4, strict=True)         try:             futures = [tpe.submit(worker, arg) for arg in chunks]             # block until all work finishes             concurrent.futures.wait(futures)         finally:             # check for exceptions in worker threads             [f.result() for f in futures]  for i in range(8):     iteration_array = np.zeros((nrows, ncols))     print(f\"{2**i} workers:\")     %time run_thread_pool(2**i)     plot_data(iteration_array) <pre>1 workers:\nCPU times: user 17 s, sys: 4.98 ms, total: 17 s\nWall time: 17 s\n</pre> <pre>2 workers:\nCPU times: user 17.1 s, sys: 1.99 ms, total: 17.1 s\nWall time: 8.54 s\n</pre> <pre>4 workers:\nCPU times: user 17.4 s, sys: 4.99 ms, total: 17.4 s\nWall time: 4.39 s\n</pre> <pre>8 workers:\nCPU times: user 18.3 s, sys: 5 ms, total: 18.3 s\nWall time: 2.31 s\n</pre> <pre>16 workers:\nCPU times: user 21 s, sys: 14 ms, total: 21 s\nWall time: 1.34 s\n</pre> <pre>32 workers:\nCPU times: user 25.3 s, sys: 17 ms, total: 25.3 s\nWall time: 858 ms\n</pre> <pre>64 workers:\nCPU times: user 34 s, sys: 26 ms, total: 34 s\nWall time: 773 ms\n</pre> <pre>128 workers:\nCPU times: user 33.7 s, sys: 30 ms, total: 33.8 s\nWall time: 770 ms\n</pre>"},{"location":"examples/mandelbrot/","title":"Mandelbrot Set Visualization","text":"<p>Note</p> <p>See the worked example notebook accompanying this page</p> <p>The Mandelbrot set is a classic example of a fractal - a mathematical structure characterized by self-similarity and spatial complexity.</p> <p>One way to compute a visualization of the set requires looping over coordinates in the complex plane corresponding to the centers of pixels in an image. Whether or not a point in the complex plane is in the mandelbrot set is a function only of the point's coordinates, so visualizing the set is very amenable to parallel speedups by breaking up the work into chunks of pixels.</p> <p>Different algorithms for set visualization range in complexity and sophistication. Here is a very basic version that calculates whether a given complex number, <code>z = x + y*1j</code>, is in the mandelbrot set. The function returns 0 for points inside the set and returns the number of iterations executed for points outside the set:</p> <pre><code>def mandelbrot(x, y, max_iterations=500):\n    z = x + y * 1j\n    p = 2\n    c = z\n    for iteration_number in range(max_iterations):\n        if abs(z) &gt;= 2:\n            return iteration_number\n        else:\n            z = z**p + c\n    else:\n        return 0\n</code></pre> <p>We can create an image of the Mandelbrot set by creating an array of pixels and assigning x and y coordinates to the center of each pixel. We can then loop over the array and call the mandelbrot function for each pixel. Let's make use of a 2D NumPy array to represent the image:</p> <pre><code>import numpy as np\n\nshape = (1000, 1000)\n\niteration_array = np.zeros(shape)\nfor i, x in enumerate(x_domain):\n    for j, y in enumerate(y_domain):\n        iteration_array[j, i] = mandelbrot(x, y)\n</code></pre> <p>This sort of \"map-reduce\" workflow, where a problem reduces to looping over a batch of calculations, is particularly amenable to parallel computation. On free-threaded Python, we can transform the simple single-threaded <code>for</code> loop above into a multithreaded parallel loop by making use of a worker function that processes a chunk of pixels:</p> <pre><code>def worker(j_y):\n    for i, x in enumerate(x_domain):\n        for j, y in j_y:\n            iteration_array[j, i] = mandelbrot(x, y)\n</code></pre> <p>The <code>worker</code> function can be called by a <code>concurrent.futures.ThreadPoolExecutor</code> and operate on a chunk of columns in the image:</p> <pre><code>def run_thread_pool(num_workers):\n    with ThreadPoolExecutor(max_workers=num_workers) as tpe:\n        chunks = itertools.batched(enumerate(y_domain), 4, strict=True)\n        try:\n            futures = [tpe.submit(worker, arg) for arg in chunks]\n            # block until all work finishes\n            concurrent.futures.wait(futures)\n        finally:\n            # check for exceptions in worker threads\n            [f.result() for f in futures]\n</code></pre> <p>In the notebook accompanying this page you can see how executing <code>run_thread_pool</code> scales well as a function of the number of worker threads up to 32 threads, the number of CPU cores available on the test machine.</p> <p>The parallel throughput of a mandelbrot visualization algorithm is limited by how much CPU power can be used to compute whether or not a point is in the set. This is an example of a \"CPU-bound\" task. Since the algorithm is implemented as a Python function that doesn't call into any libraries that release the GIL, the GIL prevents parallel execution of the <code>mandelbrot</code> function on the GIL-enabled build, and you will not see any parallel speedups running the example notebook using the GIL-enabled interpreter.</p>"},{"location":"examples/monte-carlo/","title":"Multithreaded Monte Carlo Simulation","text":"<p>Modern computer programs that play the game of Go commonly use Monte Carlo Tree Search (MCTS) as the search algorithm. Examples of programs using this technique are AlphaGo, CrazyStone and Zen. Look for \"AlphaGo - The Movie\" on YouTube if you are interested in a story about how AlphaGo won a 5 game match verses a professional Go player. Monte Carlo-based algorithms are typically good candidates for multi-threaded or multi-process parallelization.</p> <p>Michi is a minimal but relatively full-featured Go engine that uses MCTS. It was authored by Petr Baudis and released under the MIT license. We will use it as an example of how free-threaded Python can speed up programs that use multiple threads. In the case of Michi, parallelizing the computation using multiple processes also works well.</p>"},{"location":"examples/monte-carlo/#run-the-example","title":"Run the example","text":"<ol> <li> <p>To get a copy of the program, clone the following GitHub repository: https://github.com/nascheme/michi.git</p> </li> <li> <p>To run the program using multiple threads and with the GIL, use the following     command line:</p> <pre><code>uv run --python=3.14 python michi.py --force-threads tsbenchmark\n</code></pre> </li> <li> <p>To run with free-threaded Python, run the following command:</p> <pre><code>uv run --python=3.14t python michi.py --force-threads tsbenchmark\n</code></pre> </li> </ol>"},{"location":"examples/monte-carlo/#understanding-the-results","title":"Understanding the results","text":"<p>On an AMD Ryzen 5 7600X 6-core processor, the following performance is obtained:</p> Configuration Time [s] default build, threads 50.5 default build, processes 4.7 free-threaded build, threads 4.8 <p>These results show that if a problem is well-suited for multi-process parallelization, it can be the most efficient approach. In the case of Michi, the inter-process communication required consists of sending positions to the worker processes and retrieving the position evaluation. This data is small and easily marshalled.</p> <p>As expected, the multi-threaded approach with the GIL enabled has poor performance. Because the program's work is almost entirely CPU-bound, the GIL prevents multi-threading from providing any significant speed-up.</p> <p>Looking at the free-threaded build, it has a small amount of overhead compared with the multi-process approach. There are two main factors that cause this. First, the free-threaded build is generally a bit slower overall than the default build. Typically, it is about 90 to 95% of the speed, depending on the platform and program. Secondly, there is likely some contention between the threads when the Go program is running, where multiple threads are trying to use the same resource concurrently. The multi-process approach does not have this kind of contention overhead.</p> <p>Depending on the program, it can be simple to start utilizing multiple threads. In the case of Michi, the following change was all that was required. In other programs, additional changes may be required to prevent data races. An additional optimization was made to make each thread use its own random generator state.</p> <pre><code>  -        worker_pool = Pool(processes=n_workers)\n  +        if FORCE_THREADS or not sys._is_gil_enabled():\n  +            print(f'using thread pool, n = {n_workers}')\n  +            worker_pool = ThreadPool(processes=n_workers)\n  +        else:\n  +            print(f'using process pool, n = {n_workers}')\n  +            worker_pool = Pool(processes=n_workers)\n</code></pre> <p>This example demonstrates that if your problem involves a significant amount of CPU-bound calculation, Python's GIL renders multi-threading an ineffective solution. With free-threaded Python, performance when using multiple threads is comparable to a multi-process solution. In summary, if data sharing between multiple processes is not practical or efficient, free-threaded Python could be an excellent tool for unlocking the performance of multi-core CPUs.</p>"}]}